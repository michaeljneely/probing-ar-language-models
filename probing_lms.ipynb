{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "probing_lms.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZhTMOEY9e05",
        "colab_type": "text"
      },
      "source": [
        "#Probing Language Models\n",
        "\n",
        "Michael Neely & Vanessa Botha\n",
        "\n",
        "Natural Language Processing 2, Spring 2020\n",
        "\n",
        "University of Amsterdam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyFNHnAk-Ba4",
        "colab_type": "text"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "We investigate the extent to which popular recurrent and attention-based neural models trained withan auto-regressive language modeling objective can represent the hierarchical nature of language. Using word representations across model hidden layers, we test for linguistic and structural properties by training one set of diagnostic classifiers to predict POS-tag labels, and another to extract tree distances between words at the sentence level."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soQfMGkB92y4",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "am1TBuNm92Gc",
        "colab_type": "text"
      },
      "source": [
        "### Mount Google Drive storage for persistence\n",
        "\n",
        "Ensure you have two folders in the root of your Google drive:\n",
        "\n",
        "1. `probing_lms` with the NLP2 [code and data](https://github.com/jumelet/nlp2-probing-lms). Ensure the [Gulordava LSTM model](https://drive.google.com/open?id=1w47WsZcZzPyBKDn83cMNd0Hb336e-_Sy) is in `lstm` sub-folder.\n",
        "2. `probing_lms_data` where sentence representation data and diagnostic classifiers will be saved"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvZjUsDX9u-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYMZoIx6_Aha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r \"drive/My Drive/probing_lms/\" ./"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9akXcxY6-GWi",
        "colab_type": "text"
      },
      "source": [
        "###Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8BBxMgy-L0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -r probing_lms/requirements.txt\n",
        "!pip install skorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qePnngymALC4",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9az_iLWAMVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Standard library\n",
        "from collections import defaultdict, Counter\n",
        "from enum import Enum\n",
        "import itertools\n",
        "import math\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "from typing import Any, Dict, List, Optional, Set, Tuple, Union\n",
        "\n",
        "# NLP2: TA Custom Code\n",
        "from probing_lms.lstm.model import RNNModel\n",
        "\n",
        "# External imports\n",
        "from conllu import parse_incr, TokenList\n",
        "from ete3 import Tree as EteTree\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.sparse.csgraph import minimum_spanning_tree\n",
        "from scipy.stats import ttest_ind\n",
        "import seaborn as sns\n",
        "import skorch\n",
        "import skorch.helper\n",
        "from skorch.callbacks import EarlyStopping, LRScheduler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "from transformers import XLNetModel, XLNetTokenizer, GPT2Model, GPT2Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvkkxCvwFwKd",
        "colab_type": "text"
      },
      "source": [
        "### Global Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8QujTy8FrGm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PACKAGE_ROOT = \"drive/My Drive/probing_lms\"\n",
        "OUT_ROOT = \"drive/My Drive/probing_lms_data\"\n",
        "\n",
        "\n",
        "CORPUS_DATA_ROOT = os.path.join(PACKAGE_ROOT, 'data/')\n",
        "CORPUS_SAMPLE_ROOT = os.path.join(CORPUS_DATA_ROOT, 'sample/')\n",
        "\n",
        "OUT_DATA_ROOT = os.path.join(OUT_ROOT, 'data/')\n",
        "OUT_SAMPLE_DATA_ROOT = os.path.join(OUT_DATA_ROOT, 'sample/')\n",
        "OUT_MODELS_ROOT = os.path.join(OUT_ROOT, 'models/')\n",
        "OUT_RESULTS_ROOT = os.path.join(OUT_ROOT, 'results/')\n",
        "OUT_RESULTS_IMAGES_ROOT = os.path.join(OUT_RESULTS_ROOT, 'images/')\n",
        "\n",
        "USE_SAMPLE = True # If true use the small sample corpus instead of the full one\n",
        "\n",
        "TRAIN_CUTOFF = None # The full corpus has 12543 training samples, but you can introduce a cutoff if you want\n",
        "\n",
        "RUN_SANITY_CHECKS = True # If you want to jump straight into the experiments, you can skip the sanity checks\n",
        "\n",
        "# You can toggle each diagnostic classification task on/off as needed\n",
        "RUN_POS_TASK = True \n",
        "RUN_STRUCTURAL_TASK = True\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF7uQP-eCsex",
        "colab_type": "text"
      },
      "source": [
        "### Filepaths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVJ8qsh5GjFs",
        "colab_type": "text"
      },
      "source": [
        "At a minimum you must have these required data files. They can be retrieved from [here](https://github.com/jumelet/nlp2-probing-lms)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwW9ROfACt-a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GULORDAVA_LSTM_MODEL_STATE_PATH = os.path.join(PACKAGE_ROOT, 'lstm/gulordava.pt')\n",
        "LSTM_VOCABULARY_PATH = os.path.join(PACKAGE_ROOT, 'lstm/vocab.txt')\n",
        "\n",
        "TRAIN_DATA_PATH = os.path.join(CORPUS_DATA_ROOT, 'en_ewt-ud-train.conllu')\n",
        "VAL_DATA_PATH = os.path.join(CORPUS_DATA_ROOT, 'en_ewt-ud-dev.conllu')\n",
        "TEST_DATA_PATH = os.path.join(CORPUS_DATA_ROOT, 'en_ewt-ud-test.conllu')\n",
        "\n",
        "TRAIN_SAMPLE_PATH = os.path.join(CORPUS_SAMPLE_ROOT, 'en_ewt-ud-train.conllu')\n",
        "VAL_SAMPLE_PATH = os.path.join(CORPUS_SAMPLE_ROOT, 'en_ewt-ud-dev.conllu')\n",
        "TEST_SAMPLE_PATH = os.path.join(CORPUS_SAMPLE_ROOT, 'en_ewt-ud-test.conllu')\n",
        "\n",
        "for path in [\n",
        "             GULORDAVA_LSTM_MODEL_STATE_PATH,\n",
        "             LSTM_VOCABULARY_PATH,\n",
        "             TRAIN_DATA_PATH,\n",
        "             VAL_DATA_PATH,\n",
        "             TEST_DATA_PATH,\n",
        "             TRAIN_SAMPLE_PATH,\n",
        "             VAL_SAMPLE_PATH,\n",
        "             TEST_SAMPLE_PATH]:\n",
        "    assert Path(path).is_file()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8wXxofH9e06",
        "colab_type": "text"
      },
      "source": [
        "# Models\n",
        "\n",
        "In the paper, we train selective diagnostic classifiers to examine whether recurrent models and attention-based transformers models encode linguistic and structural properties differently. Recurrent models have a chain-like nature and process sentences word by word while maintaining a hidden state that summarizes past inputs. In contrast, transformer architectures process all words in the sentence at once, using self-attention to learn dependencies between words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDlksg0EIBii",
        "colab_type": "text"
      },
      "source": [
        "## Transformers\n",
        "Define a common interface for Transformer models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IJLajXJH-Sn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerModel:\n",
        "\n",
        "    def __init__(self, id: str,  model: transformers.PreTrainedModel, tokenizer: transformers.PreTrainedTokenizer):\n",
        "        self.id = id\n",
        "        self.model = model.to(DEVICE)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.hidden_layers = self.model.config.n_layer + 1 # include embeddings\n",
        "        if isinstance(model, GPT2Model):\n",
        "            self.hidden_dim = self.model.config.n_embd\n",
        "        else:\n",
        "            self.hidden_dim = self.model.config.d_model\n",
        "        self.model.eval()\n",
        "\n",
        "    def clear_model(self):\n",
        "        self.model = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ymduxogICbN",
        "colab_type": "text"
      },
      "source": [
        "## LSTM\n",
        "\n",
        "Define interface for the Gulordava et al. pre-trained LSTM from [Colorless green recurrent networks dream hierarchically.](https://arxiv.org/abs/1803.11138)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-y8VKs9gIFFq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GulordavaLSTM:\n",
        "    def __init__(self, id: str, rnn_model: RNNModel, state_path: str, vocab_path: str):\n",
        "        self.id = id\n",
        "        self.model = rnn_model.to(DEVICE)\n",
        "        self.model.load_state_dict(torch.load(state_path))\n",
        "        self.hidden_dim = self.model.nhid\n",
        "        self.vocab = self._init_vocab(vocab_path)\n",
        "        self.hidden_layers = 2 # embedding + output\n",
        "        self.model.eval()\n",
        "\n",
        "    def _init_vocab(self, vocab_path: str) -> Dict[str, int]:\n",
        "        with open(vocab_path) as f:\n",
        "            w2i = {w.strip(): i for i, w in enumerate(f)}\n",
        "\n",
        "        vocab = defaultdict(lambda: w2i[\"<unk>\"])\n",
        "        vocab.update(w2i)\n",
        "        return vocab\n",
        "\n",
        "    def clear_model(self):\n",
        "        self.model = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU4E2ryvz16T",
        "colab_type": "text"
      },
      "source": [
        "## Language Model Initialization\n",
        "\n",
        "Finally, let's write a function to quickly load whatever language model we are interested in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXjaVzmeDlGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LanguageModel = Union[TransformerModel, GulordavaLSTM]\n",
        "\n",
        "def initialize_model(model_id: str) -> LanguageModel:\n",
        "    if model_id == 'xlnet':\n",
        "        return TransformerModel(\n",
        "            id=model_id,\n",
        "            model=XLNetModel.from_pretrained('xlnet-base-cased', output_hidden_states=True),\n",
        "            tokenizer=XLNetTokenizer.from_pretrained('xlnet-base-cased'))\n",
        "    elif model_id == 'distilgpt2':\n",
        "        return TransformerModel(\n",
        "            id=model_id,\n",
        "            model=GPT2Model.from_pretrained('distilgpt2', output_hidden_states=True),\n",
        "            tokenizer=GPT2Tokenizer.from_pretrained('distilgpt2'))\n",
        "    elif model_id == 'transformer_xl':\n",
        "        return TransformerModel(\n",
        "            id=model_id,\n",
        "            model=TransfoXLModel.from_pretrained('transfo-xl-wt103', output_hidden_states=True),\n",
        "            tokenizer=TransfoXLTokenizer.from_pretrained('transfo-xl-wt103'))\n",
        "    elif model_id == 'lstm':\n",
        "        return GulordavaLSTM(\n",
        "            id=model_id,\n",
        "            rnn_model=RNNModel('LSTM', 50001, 650, 650, 2),\n",
        "            state_path = GULORDAVA_LSTM_MODEL_STATE_PATH,\n",
        "            vocab_path=LSTM_VOCABULARY_PATH)\n",
        "    else:\n",
        "        return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPB-OKt49e1D",
        "colab_type": "text"
      },
      "source": [
        "# Data\n",
        "\n",
        "We use the [Universal Dependencies English Web Treebank](https://github.com/UniversalDependencies/UD_English-EWT) with the provided train/test/validation splits, which we parse with the [conllu library](https://github.com/EmilStenstrom/conllu/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5t8Kow99e1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_corpus(filename: str) -> List[TokenList]:\n",
        "    data_file = open(filename, encoding=\"utf-8\")\n",
        "    ud_parses = list(parse_incr(data_file))\n",
        "    return ud_parses\n",
        "\n",
        "class DataFoldNames(Enum):\n",
        "    train = 'train'\n",
        "    test = 'test'\n",
        "    validation = 'validation'\n",
        "\n",
        "class Corpus:\n",
        "    def __init__(self, id: str, train_path: str, validation_path: str, test_path: str):\n",
        "        self.id = id\n",
        "        if TRAIN_CUTOFF:\n",
        "            self.train = parse_corpus(train_path)[0:TRAIN_CUTOFF]\n",
        "        else:\n",
        "            self.train = parse_corpus(train_path)\n",
        "        self.validation = parse_corpus(validation_path)\n",
        "        self.test = parse_corpus(test_path)\n",
        "        self._folds = {\n",
        "            'train': self.train,\n",
        "            'test': self.test,\n",
        "            'validation': self.validation\n",
        "        }\n",
        "        self._tokens_per_fold = {\n",
        "            'train': self._calculate_num_tokens_in_fold(self.train),\n",
        "            'test': self._calculate_num_tokens_in_fold(self.test),\n",
        "            'validation': self._calculate_num_tokens_in_fold(self.validation)\n",
        "        }\n",
        "\n",
        "    def _calculate_num_tokens_in_fold(self, fold: List[TokenList]):\n",
        "        return sum([len(parse) for parse in fold]) \n",
        "\n",
        "    def get_fold_by_name(self, fold_name: DataFoldNames):\n",
        "        return self._folds[fold_name.value]\n",
        "\n",
        "    def get_num_tokens_in_fold(self, fold_name: DataFoldNames):\n",
        "        return self._tokens_per_fold[fold_name.value]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMXnWl2QIrvc",
        "colab_type": "text"
      },
      "source": [
        "## Load Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyLN3Pcgc2HQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_id = 'sample' if USE_SAMPLE else 'full'\n",
        "\n",
        "if USE_SAMPLE:\n",
        "    CORPUS = Corpus('sample', TRAIN_SAMPLE_PATH, VAL_SAMPLE_PATH, TEST_SAMPLE_PATH)\n",
        "else:\n",
        "    CORPUS = Corpus('full', TRAIN_DATA_PATH, VAL_DATA_PATH, TEST_DATA_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NH8wzapg9e1H",
        "colab_type": "text"
      },
      "source": [
        "# Generating Representations\n",
        "\n",
        "We now have our data all set, our models are running and we are good to go!\n",
        "\n",
        "The next step is now to create the model representations for the sentences in our corpora. Once we have generated these representations we can store them, and train additional diagnostic (/probing) classifiers on top of the representations.\n",
        "\n",
        "Transformer models make use of Byte-Pair Encodings (BPE), that chunk up a piece of next in subword pieces. For example, a word such as \"largely\" could be chunked up into \"large\" and \"ly\". We are interested in probing linguistic information on the __word__-level. Therefore, we will follow the suggestion of Hewitt et al. (2019a, footnote 4), and create the representation of a word by averaging over the representations of its subwords. So the representation of \"largely\" becomes the average of that of \"large\" and \"ly\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgWdQXKm9e1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fetch_sen_reps(ud_parses: List[TokenList], language_model: Union[GulordavaLSTM, TransformerModel], hidden_layer_id: Optional[int] = None) -> List[torch.Tensor]:\n",
        "    \"\"\" Fetches hidden word-level representations from the given model. Representations at the word-level are obtained \n",
        "    by averaging the sub-representations that correspond to each particular word, per Hewitt et al. (2019a, footnote 4)\n",
        "\n",
        "    Args:\n",
        "        - ud_parses (conllu.TokenList): UD parsed .conllu file\n",
        "        - language_model (Union[GulordavaLSTM, TransformerModel]): Language Model from which hidden representations are fetched\n",
        "        - hidden_layer_id (Optional[int]) Defaults to None. If specified, only return the hidden representations at the given layer\n",
        "\n",
        "    Returns:\n",
        "        - List of tensors of shape (sequence_length, representation_size, num_hidden_layers) if hidden_layer_id is not specified else (sequence_length, representation_size)\n",
        "    \"\"\"         \n",
        "    language_model.model.eval()\n",
        "\n",
        "    is_lstm = isinstance(language_model, GulordavaLSTM)\n",
        "\n",
        "    if hidden_layer_id == -1:\n",
        "        hidden_layer_id = language_model.hidden_layers -1\n",
        "\n",
        "    tokens_in_corpus = sum([len(parse) for parse in ud_parses])\n",
        "\n",
        "    # Keep track of where to split the final tensor\n",
        "    sentence_splits = []\n",
        "\n",
        "    # Pre-allocate memory for reps\n",
        "    if hidden_layer_id is not None:\n",
        "        reps = torch.zeros((tokens_in_corpus, language_model.hidden_dim, 1), device=DEVICE)\n",
        "    else:\n",
        "        reps = torch.zeros((tokens_in_corpus, language_model.hidden_dim, language_model.hidden_layers), device=DEVICE)\n",
        "\n",
        "    # Keep track of where to insert each sentence representation into the reps tensor\n",
        "    reps_index = 0\n",
        "\n",
        "    # Build reps tensor\n",
        "    for tokenlist in tqdm(ud_parses):\n",
        "\n",
        "        # Keep track of where to average BPE sub-representations\n",
        "        track_words = []\n",
        "        num_tokens = len(tokenlist)\n",
        "        encoded_sentence = []\n",
        "\n",
        "        # Add a space at the start of a new word; only required for transformer models\n",
        "        add_prefix_space = False\n",
        "\n",
        "        # Encode based on the chunking of the treebank\n",
        "        for token_info in tokenlist:\n",
        "            \n",
        "            token = ' ' + token_info['form'] if add_prefix_space else token_info['form']\n",
        "            is_chunk = token_info.get('misc') and token_info.get('misc').get('SpaceAfter')\n",
        "            add_prefix_space = False if is_chunk or is_lstm else True\n",
        "           \n",
        "            if is_lstm:\n",
        "                input_id = language_model.vocab[token]\n",
        "                encoded_sentence.append(input_id)\n",
        "            else: \n",
        "                input_ids = language_model.tokenizer.encode(token, add_special_tokens=False)\n",
        "                encoded_sentence.extend(input_ids)                          \n",
        "                track_words.append(len(input_ids))\n",
        "                \n",
        "\n",
        "        encoded_sentence = torch.tensor([encoded_sentence], dtype=torch.long, device=DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if is_lstm:\n",
        "                # (batch_size * max_sequence_length * hidden_dim * 2)\n",
        "                hidden_state_OI = language_model.model(encoded_sentence, hidden=language_model.model.init_hidden(1))\n",
        "                # remove batch dimension -> (max_sequence_length * hidden_dim * 2)\n",
        "                sentence_rep = hidden_state_OI.squeeze(0)\n",
        "                \n",
        "                if hidden_layer_id is not None:\n",
        "                    sentence_rep = torch.index_select(sentence_rep, dim=2, index=torch.tensor([hidden_layer_id], device=DEVICE))\n",
        "            else:\n",
        "                # [-1] hidden representations are the last element in the tuple returned from a transformer forward pass\n",
        "                hidden_states = language_model.model(encoded_sentence)[-1]\n",
        "\n",
        "                if hidden_layer_id is not None:\n",
        "                    hidden_states = [hidden_states[hidden_layer_id]]\n",
        "                \n",
        "                sentence_rep = torch.tensor([], device=DEVICE)\n",
        "                for hidden in hidden_states:\n",
        "                    #average the subword representations that belong to 1 token!\n",
        "                    sentence_rep_ = torch.cat([word.mean(dim=1) for word in torch.split(hidden, track_words, dim=1)])\n",
        "                    sentence_rep_ = sentence_rep_.unsqueeze(2)\n",
        "                    sentence_rep = torch.cat((sentence_rep, sentence_rep_), dim=2)\n",
        "\n",
        "        reps[reps_index: reps_index + num_tokens] = sentence_rep\n",
        "        reps_index += num_tokens\n",
        "        sentence_splits.append(sentence_rep.shape[0])\n",
        "\n",
        "    if hidden_layer_id is not None:\n",
        "        # remove layer dimension\n",
        "        reps = reps.squeeze(-1)\n",
        "    stacked_reps = reps.to('cpu')\n",
        "    unstacked_reps = torch.split(stacked_reps, sentence_splits, dim=0)\n",
        "    return unstacked_reps\n",
        "\n",
        "def stack_sen_reps(unstacked_sen_reps: List[torch.Tensor]) -> torch.Tensor:\n",
        "    num_tokens = sum([sen_rep.shape[0] for sen_rep in unstacked_sen_reps])\n",
        "    hidden_dim = unstacked_sen_reps[0].shape[1]\n",
        "    if len(unstacked_sen_reps[0].shape) == 3:\n",
        "        hidden_layers = unstacked_sen_reps[0].shape[2]\n",
        "        stacked_reps = torch.zeros((num_tokens, hidden_dim, hidden_layers))\n",
        "    else:\n",
        "        stacked_reps = torch.zeros((num_tokens, hidden_dim))\n",
        "    return torch.cat(unstacked_sen_reps, dim=0, out=stacked_reps)\n",
        "\n",
        "# I provide the following sanity check, that compares your representations against a pickled version of mine.\n",
        "# Note that I use the DistilGPT-2 LM here. For the LSTM I used 0-valued initial states.\n",
        "def assert_sen_reps(lstm: GulordavaLSTM, distilgpt2: TransformerModel):\n",
        "    with open(os.path.join(PACKAGE_ROOT,'distilgpt2_emb1.pickle'), 'rb') as f:\n",
        "        distilgpt2_emb1 = pickle.load(f)\n",
        "        \n",
        "    with open(os.path.join(PACKAGE_ROOT,'lstm_emb1.pickle'), 'rb') as f:\n",
        "        lstm_emb1 = pickle.load(f)\n",
        "    \n",
        "    if not USE_SAMPLE:\n",
        "        first_training_sample = parse_corpus(TRAIN_SAMPLE_PATH)[:1]\n",
        "    else:\n",
        "        first_training_sample = CORPUS.train[:1]\n",
        "\n",
        "    own_distilgpt2_emb1 = stack_sen_reps(fetch_sen_reps(first_training_sample, distilgpt2, -1))\n",
        "    own_lstm_emb1 = stack_sen_reps(fetch_sen_reps(first_training_sample, lstm, -1))\n",
        "    \n",
        "    assert distilgpt2_emb1.shape == own_distilgpt2_emb1.shape\n",
        "    assert lstm_emb1.shape == own_lstm_emb1.shape\n",
        "    \n",
        "    assert torch.allclose(distilgpt2_emb1, own_distilgpt2_emb1, atol=1e-5), \"DistilGPT-2 embs don't match!\"\n",
        "    assert torch.allclose(lstm_emb1, own_lstm_emb1, atol=1e-5), \"LSTM embs don't match!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjTE1aQANimy",
        "colab_type": "text"
      },
      "source": [
        "## Saved Representation File Reading/Writing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umS0U9D9Npj1",
        "colab_type": "text"
      },
      "source": [
        "These representations take up a lot of memory. We can write the sentence representations for a particular model and corpus to disk and then load in the desired stacked or unstacked representation at the particular hidden layer we want."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcWUxCS-9DnO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ensure_dir(file_path: str) -> None:\n",
        "  '''Create a directory at the provided path if one does not already exist'''\n",
        "  directory = os.path.dirname(file_path)\n",
        "  if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "def get_saved_rep_filename(corpus_id: str, fold: DataFoldNames, language_model_id: str, hidden_layer_id: int):\n",
        "    data_root = OUT_DATA_ROOT if corpus_id != 'sample' else OUT_SAMPLE_DATA_ROOT\n",
        "    return os.path.join(data_root, f'{language_model_id}/{fold.value}_{hidden_layer_id}.pt')\n",
        "\n",
        "def reps_already_exist(corpus_id: str, language_model: LanguageModel):\n",
        "    for fold in DataFoldNames:\n",
        "        for hidden_layer_id in range(language_model.hidden_layers):\n",
        "            if not Path(get_saved_rep_filename(corpus_id, fold, language_model.id, hidden_layer_id)).is_file():\n",
        "                return False\n",
        "    return True\n",
        "\n",
        "def write_sen_reps(corpus: Corpus, language_model: LanguageModel, overwrite: Optional[bool] = False):\n",
        "    data_root = OUT_DATA_ROOT if corpus.id != 'sample' else OUT_SAMPLE_DATA_ROOT\n",
        "    pickle_root = os.path.join(data_root, language_model.id + '/')\n",
        "    ensure_dir(pickle_root)\n",
        "\n",
        "    print(f'Generating and saving sentence representations for {language_model.id}')\n",
        "    for fold in DataFoldNames:\n",
        "        print(f'Fetching sentence representations for fold {fold.value}')\n",
        "\n",
        "        for layer_id in range(language_model.hidden_layers):\n",
        "\n",
        "            print(f'Fetching sentence representations at layer {layer_id}')\n",
        "\n",
        "            unstacked_fold_sen_reps = fetch_sen_reps(corpus.get_fold_by_name(fold), language_model, layer_id)\n",
        "\n",
        "            save_path = get_saved_rep_filename(corpus.id, fold, language_model.id, layer_id)\n",
        "\n",
        "            unstacked_sen_reps_at_layer = unstacked_fold_sen_reps\n",
        "\n",
        "            with open(save_path, 'wb+') as handle:\n",
        "                torch.save(unstacked_sen_reps_at_layer, handle)\n",
        "                print(f'{language_model.id} sentence representations at hidden layer {layer_id} saved for {fold.value} fold of {corpus.id} corpus')\n",
        "\n",
        "    print(f'All {language_model.id} sentence representations saved for corpus {corpus.id}')\n",
        "\n",
        "def load_sen_reps(corpus_id: str, fold: DataFoldNames, language_model_id: str, hidden_layer_id: str, stack: Optional[bool] = False):\n",
        "    print(f'Loading the {hidden_layer_id} hidden layer {language_model_id} sentence representations for the {fold.value} fold of {corpus_id} corpus')\n",
        "    saved_sen_reps_path = get_saved_rep_filename(corpus_id, fold, language_model_id, hidden_layer_id)\n",
        "    with open(saved_sen_reps_path, 'rb') as handle:\n",
        "        sen_reps = torch.load(handle)\n",
        "        if stack:\n",
        "            sen_reps = stack_sen_reps(sen_reps)\n",
        "        return sen_reps\n",
        "\n",
        "\n",
        "def initialize_and_prepare_model(model_id: str, corpus: Corpus, clear_model: Optional[bool] = True):\n",
        "    print(f'Loading {model_id}')\n",
        "    language_model = initialize_model(model_id)\n",
        "    if not reps_already_exist(corpus.id, language_model):\n",
        "        print(f'Representations are not present on disk. Generating...')\n",
        "        write_sen_reps(corpus, language_model)\n",
        "    else:\n",
        "        print(f'Representations already exist on disk.')\n",
        "    if clear_model:\n",
        "        language_model.clear_model()\n",
        "    return language_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUZiHLfm7X3K",
        "colab_type": "text"
      },
      "source": [
        "## Generate the Data and Load the Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L79WREm5KHat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if RUN_SANITY_CHECKS:\n",
        "    LSTM = initialize_and_prepare_model('lstm', CORPUS, clear_model=False)\n",
        "else:\n",
        "    LSTM = initialize_and_prepare_model('lstm', CORPUS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0hcKT7iKDrg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if RUN_SANITY_CHECKS:\n",
        "    DISTILGPT2 = initialize_and_prepare_model('distilgpt2', CORPUS, clear_model=False)\n",
        "else:\n",
        "    DISTILGPT2 = initialize_and_prepare_model('distilgpt2', CORPUS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RIZ__fy7XS7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "XLNET = initialize_and_prepare_model('xlnet', CORPUS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPzUniHhOAld",
        "colab_type": "text"
      },
      "source": [
        "## Sanity Check: Sentence Representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWwPMASrD01X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_transformer(text: str, transformer: TransformerModel) -> torch.Tensor:\n",
        "    input_ids = [transformer.tokenizer.encode(text, add_special_tokens=True)]\n",
        "    text_data = torch.tensor(input_ids, device=DEVICE)\n",
        "    with torch.no_grad():\n",
        "        hidden_states = transformer.model(text_data)[-1]\n",
        "        return hidden_states\n",
        "\n",
        "def test_lstm(text: str, lstm: GulordavaLSTM) -> torch.Tensor:\n",
        "    input_ids = [lstm.vocab[token] for token in text.split(' ')]\n",
        "    text_data = torch.tensor(input_ids, device=DEVICE).unsqueeze(0)\n",
        "    hidden = LSTM.model.init_hidden(1)\n",
        "    with torch.no_grad():\n",
        "        hidden_states = LSTM.model(text_data, hidden)\n",
        "        return hidden_states\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1siPVDyXg1d",
        "colab_type": "text"
      },
      "source": [
        "Ensure models work as intended"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Geti_gO0XJoY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if RUN_SANITY_CHECKS:\n",
        "    test_sentence = 'I am talking to a computer'\n",
        "    expected_length = len(test_sentence.split(' '))\n",
        "\n",
        "    lstm_output = test_lstm(test_sentence, LSTM)\n",
        "    assert list(lstm_output.shape) == [1, expected_length, LSTM.hidden_dim, LSTM.hidden_layers]\n",
        "\n",
        "    distilgpt2_output = test_transformer(test_sentence, DISTILGPT2)\n",
        "    assert len(distilgpt2_output) == DISTILGPT2.hidden_layers\n",
        "    assert list(distilgpt2_output[0].shape) == [1, expected_length, DISTILGPT2.hidden_dim]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oY41Z40HXjuk",
        "colab_type": "text"
      },
      "source": [
        "Compare sentence representations to Jaap's\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzcYY9nVXXme",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if RUN_SANITY_CHECKS:\n",
        "    assert_sen_reps(LSTM, DISTILGPT2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjl5GgFdXpGk",
        "colab_type": "text"
      },
      "source": [
        "Ensure we can load sentence representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QB6te6ijXoai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if RUN_SANITY_CHECKS:\n",
        "    stacked_distilgpt2_second_hidden_layer_reps = load_sen_reps(CORPUS.id, DataFoldNames.train, DISTILGPT2.id, 0, stack=True)\n",
        "\n",
        "    assert(len(stacked_distilgpt2_second_hidden_layer_reps.shape) == 2)\n",
        "    assert stacked_distilgpt2_second_hidden_layer_reps.shape[0] == CORPUS.get_num_tokens_in_fold(DataFoldNames.train)\n",
        "    assert stacked_distilgpt2_second_hidden_layer_reps.shape[1] == DISTILGPT2.hidden_dim\n",
        "\n",
        "    del(stacked_distilgpt2_second_hidden_layer_reps)\n",
        "\n",
        "    unstacked_distilgpt2_second_hidden_layer_reps = load_sen_reps(CORPUS.id, DataFoldNames.train, DISTILGPT2.id, 1, stack=False)\n",
        "    assert len(unstacked_distilgpt2_second_hidden_layer_reps) == len(CORPUS.train)\n",
        "    assert(len(unstacked_distilgpt2_second_hidden_layer_reps[0].shape) == 2)\n",
        "\n",
        "    del(unstacked_distilgpt2_second_hidden_layer_reps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAURSkXMXtE8",
        "colab_type": "text"
      },
      "source": [
        "Cleanup: remove the pytorch models from the interfaces. We don't need them anymore\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzDWENkCXzk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if RUN_SANITY_CHECKS:\n",
        "    LSTM.clear_model()\n",
        "    DISTILGPT2.clear_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBrqUJe_0qi2",
        "colab_type": "text"
      },
      "source": [
        "# Linguistic Probe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shXdE16b9e1Q",
        "colab_type": "text"
      },
      "source": [
        "## Diagnostic Classification\n",
        "\n",
        "DCs are simple in their complexity on purpose. To read more about why this is the case you could look at the \"Designing and Interpreting Probes with Control Tasks\" by Hewitt and Liang (esp. Sec. 3.2)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FT8a05os9e1Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DIAGNOSTIC CLASSIFIER\n",
        "\n",
        "class Diagnostic_classifier(nn.Module):\n",
        "    def __init__(self, rep_dim, n_classes):\n",
        "        super(Diagnostic_classifier, self).__init__()\n",
        "\n",
        "        self.linear = nn.Linear(rep_dim, n_classes) \n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5RXj91M0vYT",
        "colab_type": "text"
      },
      "source": [
        "##  POS-tag Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmevQ0NOt5ID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FETCH POS LABELS\n",
        "\n",
        "# Should return a tensor of shape (num_tokens_in_corpus,)\n",
        "# Make sure that when fetching these pos tags for your train/dev/test corpora you share the label vocabulary.\n",
        "\n",
        "def fetch_pos_tags(ud_parses: List[TokenList], pos_vocab=None, stack=True) -> torch.Tensor:\n",
        "    \"\"\" Fetches POS-tag labels for each token in the corpus\n",
        "    Args:\n",
        "      - ud_parses (conllu.TokenList): UD parsed corpus\n",
        "      - pos_vocab (defaultdict): Encodings of the different POS-tags\n",
        "\n",
        "    Returns: \n",
        "      - Tensor of shape (num_tokens_in_corpus, ) containing encoded POS-tags\n",
        "    \"\"\"\n",
        "\n",
        "    sentence_splits = [] \n",
        "    pos_tags = []\n",
        "    \n",
        "    for tokenlist in ud_parses:\n",
        "        sentence_splits.append(len(tokenlist))\n",
        "        pos_tags.extend([token_info[\"upostag\"] for token_info in tokenlist])\n",
        "\n",
        "      \n",
        "    # Get the pos tags of all tokens in the corpus\n",
        "    # pos_tags2 = [token_info[\"upostag\"] for tokenlist in ud_parses for token_info in tokenlist]\n",
        "\n",
        "    \n",
        "    # Create pos vocabulary if not given \n",
        "    if not pos_vocab:\n",
        "        unique_pos = set(pos_tags)\n",
        "        unique_pos.add(\"X\")\n",
        "        p2i = {pos: i for i, pos in enumerate(unique_pos)}\n",
        "        pos_vocab = defaultdict(lambda: p2i[\"X\"])\n",
        "        pos_vocab.update(p2i)\n",
        "        \n",
        "    encoded_pos_tags = torch.tensor([pos_vocab[pos] for pos in pos_tags], dtype=torch.long)\n",
        "\n",
        "    if not stack:\n",
        "        encoded_pos_tags = torch.split(encoded_pos_tags, sentence_splits)     \n",
        "    \n",
        "    return encoded_pos_tags, pos_vocab  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZTLBySe0yR5",
        "colab_type": "text"
      },
      "source": [
        "## POS-tag Control Task "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE-fjroLs0Qi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_pos_empirical_distr(ud_parses: List[TokenList]): \n",
        "\n",
        "    pos_tags = [token_info[\"upostag\"] for tokenlist in ud_parses for token_info in tokenlist]\n",
        "    pos_count = Counter(pos_tags)\n",
        "\n",
        "    pos_empirical_distr = {k: v / len(pos_tags) for k, v in pos_count.items()}\n",
        "\n",
        "    return pos_empirical_distr\n",
        "\n",
        "def get_C(vocab, pos_vocab, corpus):\n",
        "    \"\"\"\n",
        "    Defines the control behavior for the POS-tagging control task, per Hewitt & Liang 2019\n",
        "    Args: \n",
        "        - vocab (defaultdict): Word vocabulary \n",
        "        - pos_vocab (defaultdict): Encodings of the different POS-tags\n",
        "    Returns: \n",
        "        - The control behavior (defaultdict) mapping of each word in the vocabulary to Y\n",
        "    \n",
        "    \"\"\" \n",
        "    pos_empirical_distr = get_pos_empirical_distr(corpus.get_fold_by_name(DataFoldNames.train))\n",
        "    probabilities = list(pos_empirical_distr.values())\n",
        "    pos_tags = list(pos_empirical_distr.keys())\n",
        "    Y = [pos_vocab[pos_tag] for pos_tag in pos_tags]\n",
        "\n",
        "    C_ = {}\n",
        "    for key in vocab.keys():\n",
        "        C_[key] = np.random.choice(Y, p=probabilities)   \n",
        "    C_[\"<unk>\"] = np.random.choice(Y, p=probabilities)   \n",
        "    C = defaultdict(lambda: C_[\"<unk>\"])\n",
        "    C.update(C_)\n",
        "    \n",
        "    return C\n",
        "\n",
        "\n",
        "def get_control_labels(ud_parses: List[TokenList], C, stack=True):\n",
        "    \"\"\"\n",
        "    Maps the tokens in the corpus using the control behavior C to obtain the labels for the control task \n",
        "    Args: \n",
        "        - ud_parses (conllu.TokenList): UD parsed corpus\n",
        "        - C (defaultdict): Control behavior\n",
        "    Returns: \n",
        "        - Tensor of shape (num_tokens_in_corpus, ) containing the control labels \n",
        "    \"\"\"\n",
        "\n",
        "    sentence_splits = [] \n",
        "    control_labels = []\n",
        "    for tokenlist in ud_parses: \n",
        "        sentence_splits.append(len(tokenlist))\n",
        "        control_labels.extend([C[token_info[\"form\"]] for token_info in tokenlist])\n",
        "\n",
        "    control_labels = torch.tensor(control_labels, dtype=torch.long)\n",
        "    if not stack:\n",
        "        control_labels = torch.split(control_labels, sentence_splits)     \n",
        "\n",
        "    return control_labels\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTkkTa_Q01Qc",
        "colab_type": "text"
      },
      "source": [
        "## Complexity Control "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IR4gtlOV1ABS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def downsample_traindata(train_x, train_y, keep_size, stack=True):\n",
        "\n",
        "    idx = np.arange(len(train_y)) \n",
        "    sample_idx = np.random.choice(idx, keep_size, replace=False)\n",
        "\n",
        "\n",
        "    ds_train_x = [sentence for i, sentence in enumerate(train_x) if i in sample_idx]\n",
        "    ds_train_y = [sentence for i, sentence in enumerate(train_y) if i in sample_idx]\n",
        "\n",
        "    if stack: \n",
        "        ds_train_x = torch.cat(ds_train_x)\n",
        "        ds_train_y = torch.cat(ds_train_y)\n",
        "\n",
        "    return ds_train_x, ds_train_y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPOrHT9D1Ca1",
        "colab_type": "text"
      },
      "source": [
        "## Experiments POS-tag Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoIPp7NhsjsD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def append_acc_per_postag(results_frame, predicted_labels, gt_labels, test_y_counts, pos_vocab):\n",
        "\n",
        "    wrong_idx = np.where(np.not_equal(predicted_labels, gt_labels))\n",
        "    wrong_per_label = Counter(gt_labels[wrong_idx])\n",
        "\n",
        "    for pos_tag, label in pos_vocab.items(): \n",
        "        if label not in gt_labels: \n",
        "            results_frame[pos_tag].append(None)\n",
        "        elif label not in wrong_per_label.keys():\n",
        "            results_frame[pos_tag].append(1)\n",
        "        else:\n",
        "            results_frame[pos_tag].append((test_y_counts[label] - wrong_per_label[label]) / test_y_counts[label])\n",
        "\n",
        "    return results_frame\n",
        "\n",
        "\n",
        "\n",
        "def run_all_results_postag(corpus, model, config, task: str = 'regular', weight_decay=[0, 0.01, 0.1, 1], seeds=[41, 42, 43]):\n",
        "    \"\"\" \n",
        "    \"\"\"\n",
        "\n",
        "    results_frame = {\"language_model_id\": [], \n",
        "                     \"hidden_layer_id\": [], \n",
        "                     \"seed\": [], \n",
        "                     \"task\": [], \n",
        "                     \"weight_decay\": [], \n",
        "                     \"sample_count\": [],\n",
        "                     \"test_accuracy\": []}\n",
        "\n",
        "    # Set skorch callbacks given in the default configurations\n",
        "    skorch_callbacks = []\n",
        "    if config[\"early_stopping\"]:\n",
        "        skorch_callbacks.append(EarlyStopping())\n",
        "    if config[\"lr_scheduler\"]:\n",
        "        skorch_callbacks.append(LRScheduler(policy=\"ReduceLROnPlateau\", mode='min', factor=0.5, patience=1))\n",
        "\n",
        "    # Add the default configurations to the lists of settings we want to run if not already in there. \n",
        "    # This is to ensure that all hyperparameter settings AND also the default settings are run \n",
        "    if config[\"weight_decay\"] not in weight_decay:\n",
        "        sample_count.append(config[\"weight_decay\"])\n",
        "\n",
        "\n",
        "    # Initialize POS labels\n",
        "    train_y_unstacked, pos_vocab = fetch_pos_tags(corpus.train, stack=False)\n",
        "    train_y = torch.cat(train_y_unstacked)\n",
        "    val_y, _= fetch_pos_tags(corpus.validation, pos_vocab)\n",
        "    test_y, _ = fetch_pos_tags(corpus.test, pos_vocab)\n",
        "\n",
        "\n",
        "    # Needed to infer the accuracies per pos tag during evaluation\n",
        "    test_y_counts = Counter(test_y.numpy())\n",
        "\n",
        "\n",
        "    # Add a column for each POS tag to the result frame\n",
        "    for pos_tag in pos_vocab.keys():\n",
        "        results_frame[pos_tag] = [] \n",
        "\n",
        "\n",
        "    # Run all the settings on all hidden layer indices \n",
        "    for hidden_layer_id in range(model.hidden_layers):\n",
        "\n",
        "        # Initialize representations \n",
        "        train_x_unstacked = load_sen_reps(corpus.id, DataFoldNames.train, model.id, hidden_layer_id, stack=False)\n",
        "        train_x = torch.cat(train_x_unstacked)\n",
        "        val_x = load_sen_reps(corpus.id, DataFoldNames.validation, model.id, hidden_layer_id, stack=True)\n",
        "        test_x = load_sen_reps(corpus.id, DataFoldNames.test, model.id, hidden_layer_id, stack=True)\n",
        "\n",
        "\n",
        "        for seed in seeds: \n",
        "            torch.manual_seed(seed)\n",
        "            np.random.seed(seed)\n",
        "\n",
        "            # Overwrite the \"true\" POS labels with control labels in the control task\n",
        "            if task==\"control\": \n",
        "                model_vocab = model.vocab if isinstance(model, GulordavaLSTM) else model.tokenizer.get_vocab()\n",
        "\n",
        "                # Get a new control behavior every seed \n",
        "                C = get_C(model_vocab, pos_vocab, corpus)\n",
        "\n",
        "                train_y_unstacked = get_control_labels(corpus.train, C, stack=False)\n",
        "                train_y = torch.cat(train_y_unstacked)\n",
        "                val_y = get_control_labels(corpus.validation, C)\n",
        "                test_y = get_control_labels(corpus.test, C)\n",
        "                test_y_counts = Counter(test_y.numpy())\n",
        "\n",
        "\n",
        "            # Initialize diagnostic classifier \n",
        "            DC_module = Diagnostic_classifier(rep_dim=model.hidden_dim, n_classes=len(pos_vocab))\n",
        "\n",
        "            \n",
        "            ###############\n",
        "            # WEIGHT DECAY\n",
        "            ###############\n",
        "            for wd in [0]:      \n",
        "                print(f'Model {model.id}')\n",
        "                print(f'Task {task}')\n",
        "                print(f'Hidden layer {hidden_layer_id}')\n",
        "                print(f'Weight decay: {wd}') \n",
        "                print(f'Training sample count: {len(train_y_unstacked)} / {len(train_y_unstacked)}') \n",
        "                print(f'Seed: {seed}')\n",
        "\n",
        "                DC_net = skorch.classifier.NeuralNetClassifier(module=DC_module, \n",
        "                                        batch_size=config[\"batch_size\"],\n",
        "                                        max_epochs=config[\"max_epochs\"],\n",
        "                                        callbacks=skorch_callbacks,\n",
        "                                        lr=config[\"lr\"],\n",
        "                                        optimizer=config[\"optimizer\"], \n",
        "                                        criterion=config[\"criterion\"],\n",
        "                                        optimizer__weight_decay=wd,\n",
        "                                        train_split=skorch.helper.predefined_split(skorch.dataset.Dataset(val_x, val_y)),\n",
        "                                        device=config[\"device\"])\n",
        "\n",
        "                DC_net.fit(train_x, train_y)\n",
        "                predicted_labels = DC_net.predict(test_x)\n",
        "                test_acc = DC_net.score(test_x, test_y)\n",
        "                print(\"Accuracy on test set: {:.4f}\\n\".format(test_acc))\n",
        "            \n",
        "\n",
        "                # Update results\n",
        "                results_frame[\"language_model_id\"].append(model.id)\n",
        "                results_frame[\"hidden_layer_id\"].append(hidden_layer_id)\n",
        "                results_frame[\"seed\"].append(seed)\n",
        "                results_frame[\"task\"].append(task)\n",
        "                results_frame[\"weight_decay\"].append(wd)\n",
        "                results_frame[\"sample_count\"].append(len(train_y_unstacked))\n",
        "                results_frame[\"test_accuracy\"].append(test_acc)\n",
        "                results_frame = append_acc_per_postag(results_frame, predicted_labels, test_y.numpy(), test_y_counts, pos_vocab)\n",
        "\n",
        "            #########################\n",
        "            # TRAINING SAMPLE COUNT #\n",
        "            #########################\n",
        "            for sc in config['training_sample_count']:\n",
        "                print(f'Model {model.id}')\n",
        "                print(f'Task {task}')\n",
        "                print(f'Hidden layer {hidden_layer_id}')\n",
        "                print(f'Weight decay: {config[\"weight_decay\"]}') \n",
        "                print(f'Training sample count: {sc} / {len(train_y_unstacked)}') \n",
        "                print(f'Seed: {seed}')\n",
        "\n",
        "\n",
        "                ds_train_x, ds_train_y = downsample_traindata(train_x_unstacked, train_y_unstacked, keep_size=sc, stack=True)\n",
        "\n",
        "                DC_net = skorch.classifier.NeuralNetClassifier(module=DC_module, \n",
        "                                        batch_size=config[\"batch_size\"],\n",
        "                                        max_epochs=config[\"max_epochs\"],\n",
        "                                        callbacks=skorch_callbacks,\n",
        "                                        lr=config[\"lr\"],\n",
        "                                        optimizer=config[\"optimizer\"], \n",
        "                                        criterion=config[\"criterion\"],\n",
        "                                        optimizer__weight_decay=config[\"weight_decay\"],\n",
        "                                        train_split=skorch.helper.predefined_split(skorch.dataset.Dataset(val_x, val_y)),\n",
        "                                        device=config[\"device\"])\n",
        "\n",
        "                DC_net.fit(ds_train_x, ds_train_y)\n",
        "                predicted_labels = DC_net.predict(test_x)\n",
        "                test_acc = DC_net.score(test_x, test_y)\n",
        "                print(\"Accuracy on test set: {:.4f}\\n\".format(test_acc))\n",
        "\n",
        "                \n",
        "                # Update results\n",
        "                results_frame[\"language_model_id\"].append(model.id)\n",
        "                results_frame[\"hidden_layer_id\"].append(hidden_layer_id)\n",
        "                results_frame[\"seed\"].append(seed)\n",
        "                results_frame[\"task\"].append(task)\n",
        "                results_frame[\"weight_decay\"].append(config[\"weight_decay\"])\n",
        "                results_frame[\"sample_count\"].append(sc)\n",
        "                results_frame[\"test_accuracy\"].append(test_acc)\n",
        "                results_frame = append_acc_per_postag(results_frame, predicted_labels, test_y.numpy(), test_y_counts, pos_vocab)\n",
        "\n",
        "\n",
        "    return pd.DataFrame(results_frame)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZRwCHu71eGW",
        "colab_type": "text"
      },
      "source": [
        "### Run All Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZczAkPcwK4I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if RUN_POS_TASK:\n",
        "\n",
        "    ensure_dir(OUT_RESULTS_ROOT)\n",
        "\n",
        "    if USE_SAMPLE:\n",
        "        training_sample_count = [1, 10, 90]\n",
        "    else:\n",
        "        training_sample_count = [100, 1000, 10000, len(CORPUS.train)]\n",
        "\n",
        "    config = {\"batch_size\": 64,\n",
        "            \"max_epochs\": 300,\n",
        "            \"lr\": 0.001,\n",
        "            \"optimizer\": torch.optim.Adam,\n",
        "            \"weight_decay\": 0,\n",
        "            \"criterion\": nn.CrossEntropyLoss,\n",
        "            \"early_stopping\": True,\n",
        "            \"lr_scheduler\": True,\n",
        "            \"training_sample_count\": training_sample_count,\n",
        "            \"device\": DEVICE\n",
        "            }\n",
        "\n",
        "    LSTM_results_regular = run_all_results_postag(CORPUS, LSTM, config, task='regular')\n",
        "    LSTM_results_regular.to_pickle(os.path.join(OUT_RESULTS_ROOT, \"LSTM_results_regular.pkl\"))\n",
        "    LSTM_results_control = run_all_results_postag(CORPUS, LSTM, config, task='control')\n",
        "    LSTM_results_control.to_pickle(os.path.join(OUT_RESULTS_ROOT,\"LSTM_results_control.pkl\"))\n",
        "\n",
        "    GPT2_results_regular = run_all_results_postag(CORPUS, DISTILGPT2, config, task='regular')\n",
        "    GPT2_results_regular.to_pickle(os.path.join(OUT_RESULTS_ROOT,\"GPT2_results_regular.pkl\"))\n",
        "    GPT2_results_control = run_all_results_postag(CORPUS, DISTILGPT2, config, task='control')\n",
        "    GPT2_results_control.to_pickle(os.path.join(OUT_RESULTS_ROOT,\"GPT2_results_control.pkl\"))\n",
        "\n",
        "    XLNET_results_regular = run_all_results_postag(CORPUS, XLNET, config, task='regular')\n",
        "    XLNET_results_regular.to_pickle(os.path.join(OUT_RESULTS_ROOT,\"XLNET_results_regular.pkl\"))\n",
        "    XLNET_results_control = run_all_results_postag(CORPUS, XLNET, config, task='control')\n",
        "    XLNET_results_control.to_pickle(os.path.join(OUT_RESULTS_ROOT,\"XLNET_results_control.pkl\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqXS6t-T1lGJ",
        "colab_type": "text"
      },
      "source": [
        "### Load and Merge Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrAzPwiZ1mce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if RUN_POS_TASK:\n",
        "\n",
        "    # Load results\n",
        "    LSTM_results_regular = pd.read_pickle(os.path.join(OUT_RESULTS_ROOT, \"LSTM_results_regular.pkl\"))\n",
        "    LSTM_results_control = pd.read_pickle(os.path.join(OUT_RESULTS_ROOT, \"LSTM_results_control.pkl\"))\n",
        "    GPT2_results_regular = pd.read_pickle(os.path.join(OUT_RESULTS_ROOT, \"GPT2_results_regular.pkl\"))\n",
        "    GPT2_results_control = pd.read_pickle(os.path.join(OUT_RESULTS_ROOT, \"GPT2_results_control.pkl\"))\n",
        "    XLNET_results_regular = pd.read_pickle(os.path.join(OUT_RESULTS_ROOT, \"XLNET_results_regular.pkl\"))\n",
        "    XLNET_results_control = pd.read_pickle(os.path.join(OUT_RESULTS_ROOT, \"XLNET_results_control.pkl\"))\n",
        "\n",
        "    # Merge results \n",
        "    ALL_POS_FRAMES = [LSTM_results_regular, LSTM_results_control, GPT2_results_regular, GPT2_results_control, XLNET_results_regular, XLNET_results_control]\n",
        "    ALL_POS_RESULTS = pd.concat(all_frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j86arhMhB6Gt",
        "colab_type": "text"
      },
      "source": [
        "### Examine Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO5l1501CCVu",
        "colab_type": "text"
      },
      "source": [
        "Plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_6BpIS2Dlui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data_sample_counts(all_results):\n",
        "    res_models_acc = {'lstm': {'mean': [], 'std':[]},\n",
        "                'distilgpt2': {'mean': [], 'std':[]},\n",
        "                'xlnet': {'mean': [], 'std':[]}}\n",
        "\n",
        "\n",
        "    res_models_selectivity = {'lstm': {'mean': [], 'std':[]},\n",
        "                'distilgpt2': {'mean': [], 'std':[]},\n",
        "                'xlnet': {'mean': [], 'std':[]}}\n",
        "\n",
        "\n",
        "    models = ['lstm', 'distilgpt2', 'xlnet']\n",
        "\n",
        "    hidden_index = [1, 6, 12]\n",
        "\n",
        "    if USE_SAMPLE:\n",
        "        scs = [1, 10, 90]\n",
        "    else:\n",
        "        scs = [100, 1000, 10000, len(CORPUS.train)]\n",
        "\n",
        "    for i, model_id in enumerate(models): \n",
        "        for sc in scs:\n",
        "            result = all_results.loc[(all_results['language_model_id'] == model_id) & (all_results['hidden_layer_id'] == hidden_index[i]) & (all_results['weight_decay'] == 0) & (all_results['sample_count'] == sc)]\n",
        "            regular = np.array(result.loc[(result['task'] == 'regular')][\"test_accuracy\"])\n",
        "\n",
        "            control = np.array(result.loc[(result['task'] == 'control')][\"test_accuracy\"])\n",
        "            \n",
        "            # technically this is generalized seletivity because the performance ceiling for the control task in this experiment is 1.0\n",
        "            selectivity = regular - control \n",
        "        \n",
        "            res_models_acc[model_id]['mean'].append(regular.mean())\n",
        "            res_models_acc[model_id]['std'].append(regular.std())\n",
        "            res_models_selectivity[model_id]['mean'].append(selectivity.mean())\n",
        "            res_models_selectivity[model_id]['std'].append(selectivity.std())\n",
        "\n",
        "    data = [res_models_acc, res_models_selectivity]\n",
        "    return data\n",
        "\n",
        "def get_data_hidden_indices(all_results):\n",
        "    res_models_acc = {'lstm': {'mean': [], 'std':[]},\n",
        "                'distilgpt2': {'mean': [], 'std':[]},\n",
        "                'xlnet': {'mean': [], 'std':[]}}\n",
        "\n",
        "\n",
        "    res_models_selectivity = {'lstm': {'mean': [], 'std':[]},\n",
        "                'distilgpt2': {'mean': [], 'std':[]},\n",
        "                'xlnet': {'mean': [], 'std':[]}}\n",
        "\n",
        "\n",
        "    models = ['lstm', 'distilgpt2', 'xlnet']\n",
        "\n",
        "    hidden_layer_indices = [np.arange(2), np.arange(7), np.arange(13)]\n",
        "    for i, model_id in enumerate(models): \n",
        "        for h in hidden_layer_indices[i]:\n",
        "            result = all_results.loc[(all_results['language_model_id'] == model_id) & (all_results['hidden_layer_id'] == h) & (all_results['weight_decay'] == 0) & (all_results['sample_count'] == 12543)]\n",
        "            regular = np.array(result.loc[(result['task'] == 'regular')][\"test_accuracy\"])\n",
        "            control = np.array(result.loc[(result['task'] == 'control')][\"test_accuracy\"])\n",
        "\n",
        "            selectivity = regular - control \n",
        "\n",
        "            res_models_acc[model_id]['mean'].append(regular.mean())\n",
        "            res_models_acc[model_id]['std'].append(regular.std())\n",
        "            res_models_selectivity[model_id]['mean'].append(selectivity.mean())\n",
        "            res_models_selectivity[model_id]['std'].append(selectivity.std())\n",
        "\n",
        "\n",
        "\n",
        "    data = [res_models_acc, res_models_selectivity]\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def plot_results(data, lims = [(0.78, 0.92), (0.15, 0.3)], step_size = [0.03, 0.03], x_label = \"Sample count\", x_ticks=[100, 1000, 10000, 12543], save_path=\"sample_count.png\"):\n",
        "\n",
        "\n",
        "    y_label = [\"Mean accuracy\", \"Mean selectivity\"]\n",
        "\n",
        "    layout=[(\"s\", \"#0077b3\"), (\"o\", \"#33cc33\"), (\"v\",\"#ff1a75\")]\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(9,3))\n",
        "\n",
        "    for i, d in enumerate(data): \n",
        "        for j, (model_id, res) in enumerate(d.items()):\n",
        "            y = np.array(res[\"mean\"])\n",
        "            std = np.array(res[\"std\"])\n",
        "\n",
        "            # Make sure does not exteed 0 - 1 \n",
        "            std_plus = y+std #np.where(y+std > 1, 1, y+std)\n",
        "            std_min = y-std #np.where(y-std < 0, 0, y-std)\n",
        "\n",
        "            marker = layout[j][0]\n",
        "            color = layout[j][1]\n",
        "\n",
        "            x = range(len(y))\n",
        "            # Plot\n",
        "            ax[i].plot(x, y, color=color, marker = marker, markersize=7, label=model_id)\n",
        "            ax[i].fill_between(x, std_min, std_plus, facecolor=color,alpha=0.2, interpolate=True)\n",
        "\n",
        "            ax[i].set_ylabel(y_label[i], fontsize=15)\n",
        "            ax[i].set_xlabel(x_label, fontsize=14)\n",
        "            ax[i].set_ylim(lims[i]) \n",
        "            ax[i].set_xlim((0, 1)) \n",
        "\n",
        "            ax[i].set_xticks(x)\n",
        "            ax[i].set_xticklabels(x_ticks, fontsize=14)\n",
        "            ax[i].set_yticks(np.arange(lims[i][0], lims[i][1]+step_size[i], step_size[i]))\n",
        "            ax[i].tick_params(axis=\"y\", labelsize=13)\n",
        "            ax[0].legend(prop={'size': 13}, loc=\"lower right\")\n",
        "    plt.suptitle(\"Part-of-Speech tag prediction\", fontsize=15)\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.93])\n",
        "    plt.savefig(save_path)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_postag_accuracies(all_results, pos_vocab, task=\"regular\", layer=\"emb\"):\n",
        "\n",
        "\n",
        "    res_models_last = {'lstm': {'mean': [], 'std':[]},\n",
        "            'distilgpt2': {'mean': [], 'std':[]},\n",
        "            'xlnet': {'mean': [], 'std':[]}}\n",
        "\n",
        "    res_models_emb = {'lstm': {'mean': [], 'std':[]},\n",
        "                'distilgpt2': {'mean': [], 'std':[]},\n",
        "                'xlnet': {'mean': [], 'std':[]}}\n",
        "\n",
        "\n",
        "    models = ['lstm', 'distilgpt2', 'xlnet']\n",
        "\n",
        "    hidden_index = [1, 6, 12]\n",
        "    _, pos_vocab = fetch_pos_tags(CORPUS.train)\n",
        "    pos_tags = [pos_tag for pos_tag in pos_vocab.keys()]\n",
        "    for i, model_id in enumerate(models): \n",
        "        for pos_tag in pos_tags:\n",
        "\n",
        "            result_last_ = all_results.loc[(all_results['language_model_id'] == model_id) & (all_results['hidden_layer_id'] == hidden_index[i]) & (all_results['weight_decay'] == 0) & (all_results['sample_count'] == 12543)]\n",
        "            result_last = np.array(result_last_.loc[(result_last_['task'] == task)][pos_tag])\n",
        "            result_emb_ = all_results.loc[(all_results['language_model_id'] == model_id) & (all_results['hidden_layer_id'] == 0) & (all_results['weight_decay'] == 0) & (all_results['sample_count'] == 12543)]\n",
        "            result_emb = np.array(result_emb_.loc[(result_emb_['task'] == task)][pos_tag])\n",
        "\n",
        "            res_models_last[model_id]['mean'].append(result_last.mean())\n",
        "            res_models_last[model_id]['std'].append(result_last.std())\n",
        "            res_models_emb[model_id]['mean'].append(result_emb.mean())\n",
        "            res_models_emb[model_id]['std'].append(result_emb.std())\n",
        " \n",
        "    \n",
        "    if layer == \"emb\":\n",
        "        data = res_models_emb\n",
        "    else:\n",
        "        data = res_models_last\n",
        "\n",
        "\n",
        "    # set width of bar\n",
        "    barWidth = 0.28\n",
        "\n",
        "    # set height of bar\n",
        "    bars1 = data['lstm'][\"mean\"]\n",
        "    bars2 = data['distilgpt2'][\"mean\"]\n",
        "    bars3 = data['xlnet'][\"mean\"]\n",
        "    \n",
        "    # Set position of bar on X axis\n",
        "    r1 = np.arange(len(bars1))\n",
        "    r2 = [x + barWidth for x in r1]\n",
        "    r3 = [x + barWidth for x in r2]\n",
        "    \n",
        "    # Make the plot\n",
        "    plt.figure(figsize=(12,5))\n",
        "    plt.bar(r1, bars1, color=\"#3392C2\", width=barWidth, edgecolor='white', yerr=data['lstm'][\"std\"], error_kw=dict(lw=0.7, capsize=4, capthick=0.7, ecolor=\"#404040\"), label='lstm')\n",
        "    plt.bar(r2, bars2, color=\"#5CD65C\", width=barWidth, edgecolor='white', yerr=data['distilgpt2'][\"std\"], error_kw=dict(lw=0.7, capsize=4, capthick=0.7, ecolor=\"#404040\"),label='distilgpt2')\n",
        "    plt.bar(r3, bars3, color=\"#FF4891\", width=barWidth, edgecolor='white', yerr=data['xlnet'][\"std\"], error_kw=dict(lw=0.7, capsize=4, capthick=0.7, ecolor=\"#404040\"), label='xlnet')\n",
        "\n",
        "    \n",
        "    # Add xticks on the middle of the group bars\n",
        "    plt.ylabel(\"Mean accuracy\", fontsize=20)\n",
        "    plt.xticks([r + barWidth for r in range(len(bars1))], pos_tags, fontsize=14, rotation=45)\n",
        "    plt.yticks(np.arange(0, 1.001, 0.1), fontsize=16)\n",
        "    plt.xlim([0,1])\n",
        "    plt.xlim([-0.4,17])\n",
        "    plt.legend(fontsize=14, framealpha=0.95, loc=\"lower right\")\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"pos_accs_\"+task+\"_\"+layer+\".png\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_empirical_distr(ud_parses: List[TokenList], save_path=\"pos_distr.png\"): \n",
        "    empirical_distr = get_pos_empirical_distr(ud_parses)\n",
        "\n",
        "    postags = sorted(list(empirical_distr.keys()))\n",
        "    probs = [empirical_distr[k ]for k in postags]\n",
        "\n",
        "\n",
        "    plt.bar(postags, probs)\n",
        "\n",
        "    # Add xticks on the middle of the group bars\n",
        "    plt.ylabel(\"Probability\", fontsize=17)\n",
        "    plt.xticks(np.arange(len(postags)), fontsize=11, rotation=45)\n",
        "    plt.yticks(np.arange(0, 0.201, 0.05), fontsize=16)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld2twMGrDnbh",
        "colab_type": "text"
      },
      "source": [
        "Plot the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbB_SqP7B4ZZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if RUN_POS_TASK:\n",
        "    print(\"WITH NORMAL SELECTIVITY\")\n",
        "    data_counts = get_data_sample_counts(ALL_POS_RESULTS)\n",
        "    data_indices = get_data_hidden_indices(ALL_POS_RESULTS)\n",
        "    plot_results(data_counts, lims = [(0.78, 0.92), (0.15, 0.3)], step_size = [0.03, 0.03], x_label = \"Sample count\", x_ticks=[100, 1000, 10000, 12543], save_path=\"sample_count.png\")\n",
        "    plot_results(data_indices, lims = [(0.81, 0.96), (-0.1, 0.3)], step_size = [0.03, 0.08], x_label= \"Hidden layer indices\", x_ticks=np.arange(13), save_path=\"hidden_indices\")\n",
        "\n",
        "    print(\"EMBEDDING LAYER\")\n",
        "    plot_postag_accuracies(ALL_POS_RESULTS, pos_vocab, task='regular', layer=\"emb\")\n",
        "    print(\"LAST HIDDEN LAYER\")\n",
        "    plot_postag_accuracies(ALL_POS_RESULTS, pos_vocab, task='regular', layer=\"last\")\n",
        "\n",
        "    print(\"EMPIRICAL DISTRIBUTION TRAIN\")\n",
        "    plot_empirical_distr(CORPUS.train, save_path=\"pos_distr_train.png\")\n",
        "    print(\"EMPIRICAL DISTRIBUTION TEST\")\n",
        "    plot_empirical_distr(CORPUS.test, save_path=\"pos_distr_test.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsQwoxtZCDWd",
        "colab_type": "text"
      },
      "source": [
        "Performance on last hidden layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJg5l5MZCIOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if RUN_POS_TASK:\n",
        "    results_lstm = all_results.loc[(ALL_POS_RESULTS['language_model_id'] == 'lstm') & (ALL_POS_RESULTS['hidden_layer_id'] == 1) & (ALL_POS_RESULTS['weight_decay'] == 0) & (ALL_POS_RESULTS['sample_count'] == 12543)]\n",
        "    regular_lstm = np.array(results_lstm.loc[(results_lstm['task'] == 'regular')][\"test_accuracy\"])\n",
        "    control_lstm = np.array(results_lstm.loc[(results_lstm['task'] == 'control')][\"test_accuracy\"])\n",
        "    selectivity_lstm = regular_lstm - control_lstm \n",
        "    print(\"LSTM performance regular task:\\n mean {}, std {}\".format(regular_lstm.mean(), regular_lstm.std()))\n",
        "    print(\"LSTM performance control task:\\n mean {}, std {}\".format(control_lstm.mean(), control_lstm.std()))\n",
        "    print(\"LSTM performance selectivity:\\n mean {}, std {}\".format(selectivity_lstm.mean(), selectivity_lstm.std()))\n",
        "    print()\n",
        "\n",
        "    results_distilgpt2 = ALL_POS_RESULTS.loc[(ALL_POS_RESULTS['language_model_id'] == 'distilgpt2') & (ALL_POS_RESULTS['hidden_layer_id'] == 6) & (ALL_POS_RESULTS['weight_decay'] == 0) & (ALL_POS_RESULTS['sample_count'] == 12543)]\n",
        "    regular_distilgpt2 = np.array(results_distilgpt2.loc[(results_distilgpt2['task'] == 'regular')][\"test_accuracy\"])\n",
        "    control_distilgpt2 = np.array(results_distilgpt2.loc[(results_distilgpt2['task'] == 'control')][\"test_accuracy\"])\n",
        "    selectivity_distilgpt2 = regular_distilgpt2 - control_distilgpt2 \n",
        "    print(\"DistilGPT2 performance regular task:\\n mean {}, std {}\".format(regular_distilgpt2.mean(), regular_distilgpt2.std()))\n",
        "    print(\"DistilGPT2 performance control task:\\n mean {}, std {}\".format(control_distilgpt2.mean(), control_distilgpt2.std()))\n",
        "    print(\"DistilGPT2 performance selectivity:\\n mean {}, std {}\".format(selectivity_distilgpt2.mean(), selectivity_distilgpt2.std()))\n",
        "    print()\n",
        "\n",
        "\n",
        "    results_xlnet = ALL_POS_RESULTS.loc[(ALL_POS_RESULTS['language_model_id'] == 'xlnet') & (ALL_POS_RESULTS['hidden_layer_id'] == 12) & (ALL_POS_RESULTS['weight_decay'] == 0) & (ALL_POS_RESULTS['sample_count'] == 12543)]\n",
        "    regular_xlnet = np.array(results_xlnet.loc[(results_xlnet['task'] == 'regular')][\"test_accuracy\"])\n",
        "    control_xlnet = np.array(results_xlnet.loc[(results_xlnet['task'] == 'control')][\"test_accuracy\"])\n",
        "    selectivity_xlnet = regular_xlnet - control_xlnet \n",
        "    print(\"XLNET performance regular task:\\n mean {}, std {}\".format(regular_xlnet.mean(), regular_xlnet.std()))\n",
        "    print(\"XLNET performance control task:\\n mean {}, std {}\".format(control_xlnet.mean(), control_xlnet.std()))\n",
        "    print(\"XLNET performance selectivity:\\n mean {}, std {}\".format(selectivity_xlnet.mean(), selectivity_xlnet.std()))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCI-vNaECLMn",
        "colab_type": "text"
      },
      "source": [
        "Statistical tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCTMLXGHCKnv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Performance on pos tag task (regular)\")\n",
        "print(\"LSTM vs. DistilGPT2: {}\".format(ttest_ind(regular_lstm, regular_distilgpt2)))\n",
        "print(\"LSTM vs. XLNET: {}\".format(ttest_ind(regular_lstm, regular_xlnet)))\n",
        "print(\"DistilGPT2 vs. XLNET: {}\".format(ttest_ind(regular_distilgpt2, regular_xlnet)))\n",
        "print()\n",
        "\n",
        "print(\"Selectivity\")\n",
        "print(\"LSTM vs. DistilGPT2: {}\".format(ttest_ind(selectivity_lstm, selectivity_distilgpt2)))\n",
        "print(\"LSTM vs. XLNET: {}\".format(ttest_ind(selectivity_lstm, selectivity_xlnet)))\n",
        "print(\"DistilGPT2 vs. XLNET: {}\".format(ttest_ind(selectivity_distilgpt2, selectivity_xlnet)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js6rw2T1mAIs",
        "colab_type": "text"
      },
      "source": [
        "# Stuctural Probe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwFCLHtW9e1S",
        "colab_type": "text"
      },
      "source": [
        "## Tree Distance Task\n",
        "\n",
        "For our gold labels, we need to recover the node distances from our parse tree. For this we will use the functionality provided by `ete3`, that allows us to compute that directly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIgN2wiA9e1T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In case you want to transform your conllu tree to an nltk.Tree, for better visualisation\n",
        "\n",
        "def rec_tokentree_to_nltk(tokentree):\n",
        "    token = tokentree.token[\"form\"]\n",
        "    tree_str = f\"({token} {' '.join(rec_tokentree_to_nltk(t) for t in tokentree.children)})\"\n",
        "\n",
        "    return tree_str\n",
        "\n",
        "\n",
        "def tokentree_to_nltk(tokentree):\n",
        "    from nltk import Tree as NLTKTree\n",
        "\n",
        "    tree_str = rec_tokentree_to_nltk(tokentree)\n",
        "\n",
        "    return NLTKTree.fromstring(tree_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOgNikoI9e1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FancyTree(EteTree):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, format=1, **kwargs)\n",
        "        \n",
        "    def __str__(self):\n",
        "        return self.get_ascii(show_internal=True)\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return str(self)\n",
        "\n",
        "\n",
        "def rec_tokentree_to_ete(tokentree):\n",
        "    idx = str(tokentree.token[\"id\"])\n",
        "    children = tokentree.children\n",
        "    if children:\n",
        "        return f\"({','.join(rec_tokentree_to_ete(t) for t in children)}){idx}\"\n",
        "    else:\n",
        "        return idx\n",
        "    \n",
        "def tokentree_to_ete(tokentree):\n",
        "    newick_str = rec_tokentree_to_ete(tokentree)\n",
        "\n",
        "    return FancyTree(f\"{newick_str};\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Av-B2sU9e1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's check if it works!\n",
        "# We can read in a corpus using the code that was already provided, and convert it to an ete3 Tree.\n",
        "\n",
        "if RUN_SANITY_CHECKS:\n",
        "    item = CORPUS.train[0]\n",
        "    tokentree = item.to_tree()\n",
        "    ete3_tree = tokentree_to_ete(tokentree)\n",
        "    print(ete3_tree)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0Ou_W7S9e1a",
        "colab_type": "text"
      },
      "source": [
        "As you can see we label a token by its token id (converted to a string). Based on these id's we are going to retrieve the node distances.\n",
        "\n",
        "To create the true distances of a parse tree in our treebank, we are going to use the `.get_distance` method that is provided by `ete3`: http://etetoolkit.org/docs/latest/tutorial/tutorial_trees.html#working-with-branch-distances\n",
        "\n",
        "We will store all these distances in a `torch.Tensor`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aamNm4Hv9e1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_gold_distances(data_fold: List[TokenList]):\n",
        "    all_distances = []\n",
        "\n",
        "    for item in tqdm(data_fold):\n",
        "        tokentree = item.to_tree()\n",
        "        ete_tree = tokentree_to_ete(tokentree)\n",
        "\n",
        "        sen_len = len(ete_tree.search_nodes())\n",
        "        distances = torch.zeros((sen_len, sen_len))\n",
        "\n",
        "        # Your code for computing all the distances comes here.\n",
        "        for node_i in ete_tree.traverse():\n",
        "            i = int(node_i.name) - 1\n",
        "            for node_j in ete_tree.traverse():\n",
        "                j = int(node_j.name) - 1\n",
        "                distances[i,j] = node_i.get_distance(node_j)\n",
        "\n",
        "        all_distances.append(distances)\n",
        "\n",
        "    return all_distances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgNBHl6W9e1c",
        "colab_type": "text"
      },
      "source": [
        "The next step is now to do the previous step the other way around. After all, we are mainly interested in predicting the node distances of a sentence, in order to recreate the corresponding parse tree.\n",
        "\n",
        "Hewitt et al. reconstruct a parse tree based on a _minimum spanning tree_ (MST, https://en.wikipedia.org/wiki/Minimum_spanning_tree). Fortunately for us, we can simply import a method from `scipy` that retrieves this MST."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByIq9E3O9e1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_mst(distances):\n",
        "    distances = torch.triu(distances).detach().numpy()\n",
        "    \n",
        "    mst = minimum_spanning_tree(distances).toarray()\n",
        "    mst[mst>0] = 1.\n",
        "    return mst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8CtPBy69e1f",
        "colab_type": "text"
      },
      "source": [
        "Let's have a look at what this looks like, by looking at a relatively short sentence in the sample corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4urC6_w9e1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if RUN_SANITY_CHECKS:\n",
        "    item = CORPUS.train[5]\n",
        "    tokentree = item.to_tree()\n",
        "    ete3_tree = tokentree_to_ete(tokentree)\n",
        "    print(ete3_tree, '\\n')\n",
        "\n",
        "    gold_distance = create_gold_distances(CORPUS.train[5:6])[0]\n",
        "    print(gold_distance, '\\n')\n",
        "\n",
        "    mst = create_mst(gold_distance)\n",
        "    print(mst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-8oFuOA9e1h",
        "colab_type": "text"
      },
      "source": [
        "Now that we are able to map edge distances back to parse trees, we can create code for our quantitative evaluation. For this we will use the Undirected Unlabeled Attachment Score (UUAS), which is expressed as:\n",
        "\n",
        "$$\\frac{\\text{number of predicted edges that are an edge in the gold parse tree}}{\\text{number of edges in the gold parse tree}}$$\n",
        "\n",
        "To do this, we will need to obtain all the edges from our MST matrix. Note that, since we are using undirected trees, that an edge can be expressed in 2 ways: an edge between node $i$ and node $j$ is denoted by both `mst[i,j] = 1`, or `mst[j,i] = 1`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsKUWlvY9e1i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def edges(mst: np.ndarray) -> Set[Tuple[int, int]]:\n",
        "    \"\"\"Extract the edges from a Minimum Spanning Tree (MST)\n",
        "\n",
        "        Args:\n",
        "            - mst (np.ndarray): MST represented as a matrix\n",
        "\n",
        "        Returns:\n",
        "            Set of node_id tuples which define edges\n",
        "    \"\"\"\n",
        "    edges = set()\n",
        "    # Your code for retrieving the edges from the MST matrix\n",
        "    edge_indices = np.argwhere(mst == 1.)\n",
        "\n",
        "    for i, j in edge_indices:\n",
        "        edges.add((i,j))\n",
        "    \n",
        "    return edges\n",
        "\n",
        "def calc_uuas(mst: np.ndarray, gold_distances: torch.Tensor) -> float:\n",
        "    \"\"\"Calculate the Undirected Unlabeled Attachment Score (UUAS) for a given MST and gold distance matrix\n",
        "\n",
        "        Args:\n",
        "            - mst (np.ndarray): MST represented as a matrix\n",
        "            - gold_distances (torch.Tensor): Tensor of distances between nodes in the gold parse tree\n",
        "\n",
        "        Returns:\n",
        "            UUAS score between 0. and 1.\n",
        "    \"\"\"\n",
        "    gold_mst = create_mst(gold_distances)\n",
        "    # The easiest way to calculate the number of predicted edges in the gold parse tree\n",
        "    # is to simply multiply the two MST matrices and count the cells that equal one\n",
        "    match_matrix = np.argwhere(mst * gold_mst == 1.)\n",
        "    gold_edges = len(edges(gold_mst))\n",
        "    if gold_edges == 0:\n",
        "        uuas = 0.0\n",
        "    else:\n",
        "        uuas = match_matrix.shape[0] / gold_edges\n",
        "\n",
        "    return uuas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gTY37FkHMpX",
        "colab_type": "text"
      },
      "source": [
        "## Tree Distance Control Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q8m3Hl7cRQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_empirical_distance_distribution(gold_distances: List[torch.Tensor]) -> Dict[int, int]:\n",
        "    distribution = {}\n",
        "    for gd in gold_distances:\n",
        "        distances = torch.triu(gd).detach().numpy()\n",
        "        for row in distances:\n",
        "            for elem in row:\n",
        "                distance = int(elem)\n",
        "                if distance != 0:\n",
        "                    if distance not in distribution:\n",
        "                        distribution[distance] = 1\n",
        "                    else:\n",
        "                        distribution[distance] += 1\n",
        "    # normalize\n",
        "    N = sum(distribution.values())\n",
        "    for k,v in distribution.items():\n",
        "        distribution[k] = v / N\n",
        "\n",
        "    return distribution"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RE1awpnLDeas",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_structural_control_behavior(train_fold: List[TokenList], empirical_distance_distribution: Dict[int, int], seed: int) -> Dict[Tuple[str, str], int]:\n",
        "    np.random.seed(seed)\n",
        "    C = {}\n",
        "    choices = list(empirical_distance_distribution.keys())\n",
        "    probs = list(empirical_distance_distribution.values())\n",
        "    for tokenlist in train_fold:\n",
        "        token_form_list = [token['form'] for token in tokenlist]\n",
        "        for i, token_i in enumerate(token_form_list):\n",
        "            if token_i not in C:\n",
        "                C[token_i] = {}\n",
        "            for j, token_j in enumerate(token_form_list):\n",
        "                if i != j:\n",
        "                    if token_j not in C[token_i]:\n",
        "                        C[token_i][token_j] = np.random.choice(choices, p=probs)\n",
        "    return C"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lENUeSzg7a4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_control_distances(ud_parses: List[TokenList], C, empirical_distance_distribution, seed: int) -> Tuple[List[torch.Tensor], float]:\n",
        "    np.random.seed(seed)\n",
        "    all_distances = []\n",
        "    oov_count = 0\n",
        "    total_pairs = 0\n",
        "    choices = list(empirical_distance_distribution.keys())\n",
        "    probs = list(empirical_distance_distribution.values())\n",
        "    for tokenlist in tqdm(ud_parses):\n",
        "        sen_len = len(tokenlist)\n",
        "        token_form_list = [token['form'] for token in tokenlist]\n",
        "        distances = torch.zeros((sen_len, sen_len))\n",
        "        for idx, token_x in enumerate(token_form_list):\n",
        "            for idy, token_y in enumerate(token_form_list):\n",
        "                if idx != idy:\n",
        "                    total_pairs += 1\n",
        "                    if token_x in C and token_y in C[token_x]:\n",
        "                        distances[idx, idy] = C[token_x][token_y]\n",
        "                    else:\n",
        "                        oov_count +=1\n",
        "                        distances[idx, idy] = np.random.choice(choices, p=probs)\n",
        "\n",
        "        all_distances.append(distances)\n",
        "    \n",
        "    # Ceiling on performance is the fraction of tokens in the evaluation set whose types occur in the training set \n",
        "    # (plus biased chance accuracy on all others tokens)\n",
        "    if oov_count == 0:\n",
        "        performance_ceiling = 1.0\n",
        "    else:\n",
        "        oov_fraction = oov_count / total_pairs\n",
        "        inv_fraction = (total_pairs - oov_count) / total_pairs\n",
        "        biased_chance_accuracy =  oov_fraction * max(empirical_distance_distribution.values())\n",
        "        performance_ceiling = inv_fraction + biased_chance_accuracy\n",
        "\n",
        "    return all_distances, performance_ceiling"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BT8btHB9e1k",
        "colab_type": "text"
      },
      "source": [
        "## Training the Structural Probes\n",
        "\n",
        "We now have everything in place to start doing the actual exciting stuff: training our structural probe!\n",
        "    \n",
        "To make life easier, we will simply take the `torch` code for this probe from John Hewitt's repository. This allows us to focus on the training regime from now on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVqQekA_9e1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class StructuralProbe(nn.Module):\n",
        "    \"\"\" Computes squared L2 distance after projection by a matrix.\n",
        "    For a batch of sentences, computes all n^2 pairs of distances\n",
        "    for each sentence in the batch.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_dim, rank, device=\"cpu\"):\n",
        "        super().__init__()\n",
        "        self.probe_rank = rank\n",
        "        self.model_dim = model_dim\n",
        "        \n",
        "        self.proj = nn.Parameter(data = torch.zeros(self.model_dim, self.probe_rank))\n",
        "        \n",
        "        nn.init.uniform_(self.proj, -0.05, 0.05)\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, batch):\n",
        "        \"\"\" Computes all n^2 pairs of distances after projection\n",
        "        for each sentence in a batch.\n",
        "        Note that due to padding, some distances will be non-zero for pads.\n",
        "        Computes (B(h_i-h_j))^T(B(h_i-h_j)) for all i,j\n",
        "        Args:\n",
        "          batch: a batch of word representations of the shape\n",
        "            (batch_size, max_seq_len, representation_dim)\n",
        "        Returns:\n",
        "          A tensor of distances of shape (batch_size, max_seq_len, max_seq_len)\n",
        "        \"\"\"\n",
        "        transformed = torch.matmul(batch, self.proj)\n",
        "        \n",
        "        batchlen, seqlen, rank = transformed.size()\n",
        "        \n",
        "        transformed = transformed.unsqueeze(2)\n",
        "        transformed = transformed.expand(-1, -1, seqlen, -1)\n",
        "        transposed = transformed.transpose(1,2)\n",
        "        \n",
        "        diffs = transformed - transposed\n",
        "        \n",
        "        squared_diffs = diffs.pow(2)\n",
        "        squared_distances = torch.sum(squared_diffs, -1)\n",
        "\n",
        "        return squared_distances\n",
        "\n",
        "    \n",
        "class L1DistanceLoss(nn.Module):\n",
        "    \"\"\"Custom L1 loss for distance matrices.\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, predictions, label_batch, length_batch):\n",
        "        \"\"\" Computes L1 loss on distance matrices.\n",
        "        Ignores all entries where label_batch=-1\n",
        "        Normalizes first within sentences (by dividing by the square of the sentence length)\n",
        "        and then across the batch.\n",
        "        Args:\n",
        "          predictions: A pytorch batch of predicted distances\n",
        "          label_batch: A pytorch batch of true distances\n",
        "          length_batch: A pytorch batch of sentence lengths\n",
        "        Returns:\n",
        "          A tuple of:\n",
        "            batch_loss: average loss in the batch\n",
        "            total_sents: number of sentences in the batch\n",
        "        \"\"\"\n",
        "        labels_1s = (label_batch != -1).float()\n",
        "        predictions_masked = predictions * labels_1s\n",
        "        labels_masked = label_batch * labels_1s\n",
        "        total_sents = torch.sum((length_batch != 0)).float()\n",
        "        squared_lengths = length_batch.pow(2).float()\n",
        "\n",
        "        if total_sents > 0:\n",
        "            loss_per_sent = torch.sum(torch.abs(predictions_masked - labels_masked), dim=(1,2))\n",
        "            normalized_loss_per_sent = loss_per_sent / squared_lengths\n",
        "            batch_loss = torch.sum(normalized_loss_per_sent) / total_sents\n",
        "        \n",
        "        else:\n",
        "            batch_loss = torch.tensor(0.0)\n",
        "        \n",
        "        return batch_loss, total_sents\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQDbcqAy21Mf",
        "colab_type": "text"
      },
      "source": [
        "### Custom PyTorch Dataset and Collate Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swvfhNyjNquD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TreeDistanceDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, target, performance_ceiling: float = 1.0):\n",
        "        self.data = data\n",
        "        self.target = target\n",
        "        self.performance_ceiling = performance_ceiling\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.target[index]\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "def tree_distance_collate(batch):\n",
        "    batch_len = len(batch)\n",
        "\n",
        "    batch_data = [item[0] for item in batch] # (seq_len x rep_dim)\n",
        "\n",
        "    batch_target = [item[1] for item in batch] # (seq_len x seq_len)\n",
        "\n",
        "    representation_dim = batch_data[0].shape[1]\n",
        "\n",
        "    max_seq_len = max(data.shape[0] for data in batch_data)\n",
        "\n",
        "    padded_data_tensor = torch.zeros((batch_len, max_seq_len, representation_dim), device=DEVICE)\n",
        "    padded_target_tensor = torch.zeros((batch_len, max_seq_len, max_seq_len), device=DEVICE).fill_(-1)\n",
        "    length_tensor = torch.zeros(batch_len, device=DEVICE)\n",
        "\n",
        "    for idx, (x, y) in enumerate(zip(batch_data, batch_target)):\n",
        "        seq_len = x.shape[0]\n",
        "        target_len = y.shape[0]\n",
        "        padded_data_tensor[idx, :seq_len] = x\n",
        "        padded_target_tensor[idx, :target_len, :target_len] = y\n",
        "        length_tensor[idx] = seq_len\n",
        "\n",
        "    return padded_data_tensor, padded_target_tensor, length_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOepQPw09e1n",
        "colab_type": "text"
      },
      "source": [
        "### Probe and Training Configuration Wrappers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cn5bYB5tBvuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TrainingConfig:\n",
        "    def __init__(self, corpus: Corpus, language_model_ids: List[str], ranks: List[int], seeds: List[int] = [67, 12484, 12], device = DEVICE,\n",
        "                 learning_rate = 10e-4, batch_size = 64, num_epochs = 25, patience = 5, overwrite: bool = False):\n",
        "        self.corpus = corpus\n",
        "        self.language_model_ids = language_model_ids\n",
        "        self.ranks = ranks\n",
        "        self.seeds = seeds\n",
        "        self.device = device\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.num_epochs = num_epochs\n",
        "        self.patience = patience\n",
        "        self.overwrite = overwrite\n",
        "\n",
        "        self.data_root = OUT_DATA_ROOT if self.corpus.id != 'sample' else OUT_SAMPLE_DATA_ROOT\n",
        "\n",
        "        self.train_gold_distances = self._load_or_create_gold_distances(DataFoldNames.train)\n",
        "        self.test_gold_distances = self._load_or_create_gold_distances(DataFoldNames.test)\n",
        "        self.validation_gold_distances = self._load_or_create_gold_distances(DataFoldNames.validation)\n",
        "        \n",
        "        self.empirical_distance_distribution = create_empirical_distance_distribution(self.train_gold_distances)\n",
        "        self.control_behavior = self._load_or_create_control_behavior()\n",
        "\n",
        "        self.train_control_distances = self._load_or_create_control_distances(DataFoldNames.train)\n",
        "        self.test_control_distances = self._load_or_create_control_distances(DataFoldNames.test)\n",
        "        self.validation_control_distances = self._load_or_create_control_distances(DataFoldNames.validation)\n",
        "\n",
        "\n",
        "    def _load_or_create_gold_distances(self, fold: DataFoldNames):\n",
        "        gd_path = os.path.join(self.data_root, f'{fold.value}_gold_distances.pt') \n",
        "        if Path(gd_path).is_file() and not self.overwrite:\n",
        "            print(f'Gold distances present on disk for {fold.value} of {self.corpus.id} corpus')\n",
        "            with open(gd_path, 'rb') as handle:\n",
        "                gold_distances = torch.load(handle)\n",
        "        else:\n",
        "            print(f'Creating gold distances for {fold.value} of {self.corpus.id} corpus and saving to disk...')\n",
        "            gold_distances = create_gold_distances(self.corpus.get_fold_by_name(fold))\n",
        "            with open(gd_path, 'wb+') as handle:\n",
        "                torch.save(gold_distances, handle)\n",
        "        return gold_distances\n",
        "\n",
        "\n",
        "    def _load_or_create_control_behavior(self):\n",
        "        control_behavior = {}\n",
        "        for seed in self.seeds:\n",
        "            cb_path = os.path.join(self.data_root, f'control_behavior_{seed}.pkl')\n",
        "            if Path(cb_path).is_file() and not self.overwrite:\n",
        "                print(f'Structural control behavior with seed {seed} present on disk')\n",
        "                with open(cb_path, 'rb') as handle:\n",
        "                    C = pickle.load(handle)\n",
        "            else:\n",
        "                print(f'Creating structural control behavior with seed {seed} and saving to disk...')\n",
        "                C = create_structural_control_behavior(self.corpus.train, self.empirical_distance_distribution, seed)\n",
        "                with open(cb_path, 'wb') as handle:\n",
        "                    pickle.dump(C, handle)\n",
        "            control_behavior[seed] = C\n",
        "        return control_behavior\n",
        "\n",
        "    def _load_or_create_control_distances(self, fold):\n",
        "        control_distances_for_seed = {}\n",
        "        for seed in self.seeds:\n",
        "            cd_path = os.path.join(self.data_root, f'{fold.value}_control_distances_{seed}.pt') \n",
        "            if Path(cd_path).is_file() and not self.overwrite:\n",
        "                print(f'Control distances with seed {seed} present on disk for {fold.value} fold of {self.corpus.id} corpus')\n",
        "                with open(cd_path, 'rb') as handle:\n",
        "                    control_distances, performance_ceiling = torch.load(handle)\n",
        "                    performance_ceiling = float(performance_ceiling)\n",
        "            else:\n",
        "                print(f'Creating control distances with seed {seed} for {fold.value} fold of {self.corpus.id} corpus and saving to disk...')\n",
        "                control_distances, performance_ceiling = create_control_distances(self.corpus.get_fold_by_name(fold), self.control_behavior[seed], self.empirical_distance_distribution, seed)\n",
        "                with open(cd_path, 'wb+') as handle:\n",
        "                    torch.save([control_distances, torch.tensor([performance_ceiling])], handle)\n",
        "            \n",
        "            control_distances_for_seed[seed] = (control_distances, performance_ceiling)\n",
        "        return control_distances_for_seed\n",
        "\n",
        "class ProbeConfig:\n",
        "\n",
        "    def __init__(self,language_model_id: str, corpus_id: str, task: str, hidden_layer_id: int, dim: int, rank: int, seed: int):\n",
        "        self.language_model_id = language_model_id\n",
        "        self.corpus_id = corpus_id\n",
        "        self.task = task\n",
        "        self.hidden_layer_id = hidden_layer_id\n",
        "        self.dim = dim\n",
        "        self.rank = rank\n",
        "        self.seed = seed\n",
        "        self.save_path = self._get_save_path()\n",
        "        self.results_path = self._get_results_path()\n",
        "\n",
        "    @staticmethod\n",
        "    def from_save_path(save_path: str):\n",
        "        _, filename = os.path.split(probe_state_path)\n",
        "        param_string, extension = os.path.splitext(filename)\n",
        "        language_model_id, corpus_id, task, hidden_layer_id, dim, rank, seed = param_string.split('_')\n",
        "        return ProbeConfig(\n",
        "            language_model_id=language_model_id,\n",
        "            corpus_id=corpus_id,\n",
        "            task=task,\n",
        "            hidden_layer_id=hidden_layer_id,\n",
        "            dim=dim,\n",
        "            rank=rank,\n",
        "            seed=seed\n",
        "        )\n",
        "\n",
        "    def was_already_trained(self) -> bool:\n",
        "        has_trained_model = Path(self.save_path).is_file()\n",
        "        has_test_set_results = Path(self.results_path).is_file()\n",
        "        return has_trained_model and has_test_set_results\n",
        "\n",
        "    def print_start_message(self) -> None:\n",
        "        print(\n",
        "            f'Training Structural Probe on {self.task} task with rank {self.rank}'\n",
        "            f' on hidden layer {self.hidden_layer_id} of {self.language_model_id}'\n",
        "            f' using {self.corpus_id} corpus data and seed {self.seed}'\n",
        "        )\n",
        "\n",
        "    def save_test_set_results(self, dataframe):\n",
        "        dataframe.to_pickle(self.results_path)\n",
        "\n",
        "    def load_test_set_results(self):\n",
        "        return pd.read_pickle(self.results_path)\n",
        "\n",
        "    def _get_param_string(self):\n",
        "        param_string = f'{self.language_model_id}_{self.corpus_id}_{self.task}_{self.hidden_layer_id}_{self.dim}_{self.rank}_{self.seed}'\n",
        "        return param_string\n",
        "\n",
        "    def _get_save_path(self):\n",
        "        ensure_dir(OUT_MODELS_ROOT)\n",
        "        param_string = self._get_param_string()\n",
        "        out_path = os.path.join(OUT_MODELS_ROOT, param_string + '.pth')\n",
        "        return out_path\n",
        "\n",
        "    def _get_results_path(self):\n",
        "        ensure_dir(OUT_RESULTS_ROOT)\n",
        "        param_string = self._get_param_string()\n",
        "        out_path = os.path.join(OUT_RESULTS_ROOT, param_string + '.pkl')\n",
        "        return out_path\n",
        "\n",
        "\n",
        "def all_results_for_hidden_layer_exist(language_model, corpus_id, hidden_layer_id, ranks, seeds):\n",
        "    for task in ['regular', 'control']:\n",
        "        for rank in ranks:\n",
        "            for seed in seeds:\n",
        "                config = ProbeConfig(\n",
        "                    language_model_id=language_model.id,\n",
        "                    corpus_id=corpus_id,\n",
        "                    task=task,\n",
        "                    hidden_layer_id=hidden_layer_id,\n",
        "                    dim=language_model.hidden_dim,\n",
        "                    rank=rank,\n",
        "                    seed=seed)\n",
        "                has_saved_model = Path(config.save_path).is_file()\n",
        "                has_saved_results = Path(config.results_path).is_file()\n",
        "                if not has_saved_model or not has_saved_results:\n",
        "                    return False\n",
        "    return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUhvVmk26KFp",
        "colab_type": "text"
      },
      "source": [
        "### Custom Training/Validation/Testing Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaPIJA1m9e1n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_batch(probe: StructuralProbe, optimizer: torch.optim.Optimizer, loss_function: L1DistanceLoss, data: torch.Tensor,\n",
        "                target: torch.Tensor, lengths: torch.Tensor) -> float:\n",
        "    optimizer.zero_grad()\n",
        "    predicted_distances = probe(data)\n",
        "    loss, _ = loss_function.forward(predicted_distances, target, lengths)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def train_epoch(probe: StructuralProbe, optimizer: torch.optim.Optimizer, loss_function: L1DistanceLoss,\n",
        "                train_loader: torch.utils.data.DataLoader, device: torch.device, num_batches: int):\n",
        "    probe.train()\n",
        "    losses = np.zeros(num_batches, dtype=float) # pre-allocate memory\n",
        "    for batch_idx, (data, target, lengths) in enumerate(train_loader):\n",
        "        data, target, lengths = data.to(device), target.to(device), lengths.to(device)\n",
        "        loss = train_batch(probe, optimizer, loss_function, data, target, lengths)\n",
        "        losses[batch_idx] = (loss)\n",
        "        if batch_idx + 1 == num_batches:\n",
        "            return losses\n",
        "\n",
        "def evaluate_batch(predicted_distances: torch.Tensor, target: torch.Tensor, lengths: torch.Tensor):\n",
        "    predicted_distances = torch.split(predicted_distances, [1 for _ in range(lengths.shape[0])], dim=0)\n",
        "    gold_distances = torch.split(target, [1 for _ in range(lengths.shape[0])], dim=0)\n",
        "    sentence_lengths = [int(item) for item in lengths.tolist()]\n",
        "\n",
        "    msts = []\n",
        "    for (pd, length) in zip(predicted_distances, sentence_lengths):\n",
        "        a = pd.squeeze(0) # remove batch dimension\n",
        "        mst = create_mst(a[:length, :length])\n",
        "        msts.append(mst)\n",
        "    \n",
        "    uuas_scores = []\n",
        "    for (mst, gd, length) in zip(msts, gold_distances, sentence_lengths):\n",
        "        a = gd.squeeze(0) # remove batch dimension\n",
        "        uuas = calc_uuas(mst, a[:length, :length])\n",
        "        uuas_scores.append(uuas)\n",
        "    return uuas_scores\n",
        "\n",
        "def validate(probe: StructuralProbe, loss_function: L1DistanceLoss, split_loader: torch.utils.data.DataLoader, device: torch.device) -> Tuple[float, float]:\n",
        "    probe.eval()\n",
        "    batch_uuas_scores = []\n",
        "    losses = []\n",
        "    with torch.no_grad():  \n",
        "        for batch_idx, (data, target, lengths) in enumerate(split_loader):\n",
        "            data, target, lengths = data.to(device), target.to(device), lengths.to(device)\n",
        "            predicted_distances = probe(data)\n",
        "            loss, _ = loss_function.forward(predicted_distances, target, lengths)\n",
        "            iter_loss = loss.item()\n",
        "            losses.append(iter_loss)\n",
        "            predicted_distances, target, lengths = predicted_distances.to('cpu'), target.to('cpu'), lengths.to('cpu')\n",
        "            batch_uuas_score = evaluate_batch(predicted_distances, target, lengths)\n",
        "            batch_uuas_scores.append(np.mean(batch_uuas_score))\n",
        "    avg_total_uuas = np.mean(batch_uuas_scores)\n",
        "    avg_total_loss = np.mean(losses)\n",
        "    return avg_total_loss, avg_total_uuas\n",
        "\n",
        "def test_probe(probe: StructuralProbe, ud_parses: List[TokenList], dataset: TreeDistanceDataset, device: torch.device) -> pd.DataFrame:\n",
        "    test_set_results = {\n",
        "        'sentence': [token_list.metadata['text'] for token_list in ud_parses],\n",
        "        'gold_distances': [gold_distance.numpy() for gold_distance in dataset.target],\n",
        "        'predicted_distances': [],\n",
        "        'uuas_score': [],\n",
        "        'corrected_uuas_score': [] # uuas score after dividing by the performance ceiling\n",
        "    }\n",
        "\n",
        "    test_dl = torch.utils.data.DataLoader(dataset=dataset, batch_size=64, shuffle=False, collate_fn=tree_distance_collate)\n",
        "    probe.eval()\n",
        "    with torch.no_grad():  \n",
        "        for batch_idx, (data, target, lengths) in enumerate(test_dl):\n",
        "            data, target, lengths = data.to(device), target.to(device), lengths.to(device)\n",
        "            predicted_distances = probe(data)\n",
        "            predicted_distances, target, lengths = predicted_distances.to('cpu'), target.to('cpu'), lengths.to('cpu')\n",
        "            batch_uuas_scores = evaluate_batch(predicted_distances, target, lengths)\n",
        "            corrected_batch_uuas_scores = np.divide(batch_uuas_scores, dataset.performance_ceiling)\n",
        "            test_set_results['predicted_distances'].extend([predicted_distance.numpy() for predicted_distance in predicted_distances])\n",
        "            test_set_results['uuas_score'].extend(batch_uuas_scores)\n",
        "            test_set_results['corrected_uuas_score'].extend(corrected_batch_uuas_scores)\n",
        "\n",
        "    return pd.DataFrame(test_set_results)\n",
        "\n",
        "def train_probe(datasets, probe_config: ProbeConfig, training_config: TrainingConfig) -> float:\n",
        "    train_dataset, test_dataset, validation_dataset = datasets\n",
        "\n",
        "    train_dl = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=training_config.batch_size, shuffle=True, collate_fn=tree_distance_collate)\n",
        "    \n",
        "    validation_dl = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=training_config.batch_size, shuffle=True, collate_fn=tree_distance_collate)\n",
        "\n",
        "    torch.manual_seed(probe_config.seed)\n",
        "    np.random.seed(probe_config.seed)\n",
        "\n",
        "    probe = StructuralProbe(probe_config.dim, probe_config.rank).to(training_config.device)\n",
        "    optimizer = torch.optim.Adam(probe.parameters(), lr=training_config.learning_rate)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5,patience=1)\n",
        "    loss_function =  L1DistanceLoss()\n",
        "\n",
        "    best_validation_uuas = 0.0\n",
        "    best_epoch_id = 0\n",
        "    counter = 0\n",
        "\n",
        "    batches_per_epoch = max(1, math.ceil(len(train_dataset.data) / training_config.batch_size))\n",
        "\n",
        "    for epoch in range(training_config.num_epochs):\n",
        "        epoch_losses = train_epoch(probe, optimizer, loss_function, train_dl, training_config.device, batches_per_epoch)\n",
        "        avg_epoch_loss = np.mean(epoch_losses)\n",
        "        validation_loss, validation_uuas = validate(probe, loss_function, validation_dl, training_config.device)\n",
        "        adjusted_validation_uuas = validation_uuas / validation_dataset.performance_ceiling\n",
        "        scheduler.step(validation_loss)\n",
        "        print(f'Epoch {epoch}: Loss: {avg_epoch_loss}, Validation UUAS: {validation_uuas}, Adjusted Validation UUAS: {adjusted_validation_uuas}')\n",
        "\n",
        "        if validation_uuas > best_validation_uuas:\n",
        "            best_validation_uuas = validation_uuas\n",
        "            best_epoch_id = epoch\n",
        "            counter = 0\n",
        "            print('Best validation UUAS so far!')\n",
        "            torch.save(probe.state_dict(), probe_config.save_path)\n",
        "        else:\n",
        "            counter += 1\n",
        "            print(f'No improvement in validation UUAS. Patience remaining: {training_config.patience - counter}')\n",
        "\n",
        "        if counter >= training_config.patience:\n",
        "            print(f'Terminated. Best Model found at epoch {best_epoch_id}')\n",
        "            break\n",
        "\n",
        "    probe.load_state_dict(torch.load(probe_config.save_path))\n",
        "    test_set_results = test_probe(probe, CORPUS.test, test_dataset, training_config.device)\n",
        "    probe_config.save_test_set_results(test_set_results)\n",
        "\n",
        "    return test_set_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq6lxItlbFgj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_all_structural_probes(training_config: TrainingConfig) -> pd.DataFrame:\n",
        "\n",
        "    print('Loading or creating gold and control distances')\n",
        "\n",
        "    final_results_frame = {\n",
        "        'language_model_id': [],\n",
        "        'hidden_layer_id': [],\n",
        "        'rank': [],\n",
        "        'seed': [],\n",
        "        'task': [],\n",
        "        'uuas_score': [],\n",
        "        'corrected_uuas_score': [],\n",
        "        'selectivity': [],\n",
        "        'generalized_selectivity': []\n",
        "    }\n",
        "\n",
        "    final_results_path = os.path.join(OUT_RESULTS_ROOT, f'final_results_{training_config.corpus.id}.pkl')\n",
        "\n",
        "    if Path(final_results_path).is_file() and not training_config.overwrite:\n",
        "        print('Final results already exist!')\n",
        "        final_results = pd.read_pickle(final_results_path)\n",
        "        return final_results\n",
        "\n",
        "    else:\n",
        "        language_models = {}\n",
        "        for language_model_id in training_config.language_model_ids:\n",
        "            print(f'Initializing and preparing {language_model_id} language model')\n",
        "            language_models[language_model_id] = initialize_and_prepare_model(language_model_id, training_config.corpus)\n",
        "\n",
        "        for seed in training_config.seeds:\n",
        "\n",
        "            test_performance_ceiling = training_config.test_control_distances[seed][1]\n",
        "            validation_performance_ceiling = training_config.validation_control_distances[seed][1]\n",
        "\n",
        "            for language_model_id in training_config.language_model_ids:\n",
        "\n",
        "\n",
        "                language_model = language_models[language_model_id]\n",
        "\n",
        "                for hidden_layer_id in range(language_model.hidden_layers):\n",
        "\n",
        "                    if not all_results_for_hidden_layer_exist(language_model, CORPUS.id, hidden_layer_id, training_config.ranks, training_config.seeds) or training_config.overwrite:\n",
        "\n",
        "                        layer_train_sen_reps = load_sen_reps(CORPUS.id, DataFoldNames.train, language_model.id, hidden_layer_id, False)\n",
        "                        layer_test_sen_reps = load_sen_reps(CORPUS.id, DataFoldNames.test, language_model.id, hidden_layer_id, False)\n",
        "                        layer_validation_sen_reps = load_sen_reps(CORPUS.id, DataFoldNames.validation, language_model.id, hidden_layer_id, False)\n",
        "\n",
        "                        regular_train_dataset = TreeDistanceDataset(layer_train_sen_reps, training_config.train_gold_distances)\n",
        "                        regular_test_dataset = TreeDistanceDataset(layer_test_sen_reps, training_config.test_gold_distances, test_performance_ceiling)\n",
        "                        regular_validation_dataset = TreeDistanceDataset(layer_validation_sen_reps, training_config.validation_gold_distances, validation_performance_ceiling)\n",
        "\n",
        "                        control_train_dataset = TreeDistanceDataset(layer_train_sen_reps, training_config.train_control_distances[seed][0], training_config.train_control_distances[seed][1])\n",
        "                        control_test_dataset = TreeDistanceDataset(layer_test_sen_reps, training_config.test_control_distances[seed][0], test_performance_ceiling)\n",
        "                        control_validation_dataset = TreeDistanceDataset(layer_validation_sen_reps, training_config.validation_control_distances[seed][0], validation_performance_ceiling)\n",
        "\n",
        "                    else:\n",
        "                        print(f'All trained models and results have been saved for hidden layer {hidden_layer_id} or {language_model.id}. Not loading dataset...')\n",
        "\n",
        "                    for rank in training_config.ranks:\n",
        "\n",
        "                        print('\\n\\n')\n",
        "                        print('#'*140)\n",
        "                        print('\\n\\n')\n",
        "\n",
        "                        # REGULAR TASK\n",
        "                        regular_probe_config = ProbeConfig(\n",
        "                            language_model_id=language_model.id,\n",
        "                            corpus_id=CORPUS.id,\n",
        "                            task='regular',\n",
        "                            hidden_layer_id=hidden_layer_id,\n",
        "                            dim = language_model.hidden_dim,\n",
        "                            rank=rank,\n",
        "                            seed=seed)\n",
        "                        \n",
        "                        regular_probe_config.print_start_message()\n",
        "                        \n",
        "                        if regular_probe_config.was_already_trained() and not training_config.overwrite:\n",
        "                            print('A probe with the given configuration has already been trained and saved to disk.')\n",
        "                            regular_results = regular_probe_config.load_test_set_results()\n",
        "                        else:\n",
        "                            regular_results = train_probe([regular_train_dataset, regular_test_dataset, regular_validation_dataset], regular_probe_config, training_config)\n",
        "                        \n",
        "                        regular_uuas = np.mean(regular_results['uuas_score'])\n",
        "                        final_results_frame['language_model_id'].append(language_model.id)\n",
        "                        final_results_frame['hidden_layer_id'].append(hidden_layer_id)\n",
        "                        final_results_frame['rank'].append(rank)\n",
        "                        final_results_frame['seed'].append(seed)\n",
        "                        final_results_frame['task'].append('regular')\n",
        "                        final_results_frame['uuas_score'].append(regular_uuas)\n",
        "                        final_results_frame['corrected_uuas_score'].append(regular_uuas) # not needed but prevents empty cells in the frame\n",
        "\n",
        "                        # CONTROL TASK\n",
        "                        control_probe_config = ProbeConfig(\n",
        "                            language_model_id=language_model.id,\n",
        "                            corpus_id=CORPUS.id,\n",
        "                            task='control',\n",
        "                            hidden_layer_id=hidden_layer_id,\n",
        "                            dim = language_model.hidden_dim,\n",
        "                            rank=rank,\n",
        "                            seed=seed)\n",
        "\n",
        "                        control_probe_config.print_start_message()\n",
        "\n",
        "                        if control_probe_config.was_already_trained() and not training_config.overwrite:\n",
        "                            print('A Probe with the given configuration has already been trained and saved to disk.')\n",
        "                            control_results = control_probe_config.load_test_set_results()\n",
        "                        else:\n",
        "                            control_results  = train_probe([control_train_dataset, control_test_dataset, control_validation_dataset], control_probe_config, training_config)\n",
        "                        \n",
        "                        control_uuas = np.mean(control_results['uuas_score'])\n",
        "                        corrected_control_uuas = np.mean(control_results['corrected_uuas_score'])\n",
        "\n",
        "                        final_results_frame['language_model_id'].append(language_model.id)\n",
        "                        final_results_frame['hidden_layer_id'].append(hidden_layer_id)\n",
        "                        final_results_frame['rank'].append(rank)\n",
        "                        final_results_frame['seed'].append(seed)\n",
        "                        final_results_frame['task'].append('control')\n",
        "                        final_results_frame['uuas_score'].append(control_uuas)\n",
        "                        final_results_frame['corrected_uuas_score'].append(corrected_control_uuas)\n",
        "\n",
        "                        selectivity = regular_uuas - control_uuas\n",
        "                        generalized_selectivity = regular_uuas - corrected_control_uuas\n",
        "\n",
        "                        final_results_frame['selectivity'].extend([selectivity, selectivity])\n",
        "                        final_results_frame['generalized_selectivity'].extend([generalized_selectivity, generalized_selectivity])\n",
        "                \n",
        "                        print(f'With the given configuration, the final selectivity of the Probe is: {selectivity}, Generalized: {generalized_selectivity}')\n",
        "        final_results = pd.DataFrame(final_results_frame)\n",
        "        final_results.to_pickle(final_results_path)\n",
        "        return final_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE9sk3xXmQuL",
        "colab_type": "text"
      },
      "source": [
        "## Run the Structural Probe Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Fafioktffm3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if RUN_STRUCTURAL_TASK:\n",
        "    STRUCTURAL_PROBE_TRAINING_CONFIG = TrainingConfig(\n",
        "        corpus=CORPUS,\n",
        "        language_model_ids=['lstm', 'distilgpt2', 'xlnet'],\n",
        "        ranks=[1, 2, 4, 8, 16, 32, 64, 128, 256, 512],\n",
        "        seeds=[67, 12484, 12],\n",
        "        overwrite=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxQwdTqTcebo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if RUN_STRUCTURAL_TASK:\n",
        "    STRUCTURAL_PROBE_RESULTS = train_all_structural_probes(STRUCTURAL_PROBE_TRAINING_CONFIG)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k2IKJ0EmShI",
        "colab_type": "text"
      },
      "source": [
        "## Examine Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBdzLlHJcsIA",
        "colab_type": "text"
      },
      "source": [
        "Let's visualize the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEUAY2q0eNZ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def graph_y_by_rank(results_frame, y_id, y_label, training_config, agg_function):\n",
        "    ensure_dir(OUT_RESULTS_IMAGES_ROOT)\n",
        "    if agg_function == 'mean':\n",
        "        grouped = results_frame.groupby(['language_model_id', 'rank', 'seed'])[y_id].mean().reset_index()\n",
        "    elif agg_function == 'max':\n",
        "        grouped = results_frame.groupby(['language_model_id', 'rank', 'seed'])[y_id].max().reset_index()\n",
        "    plt.clf()\n",
        "    ax = sns.lineplot(x='rank', y=y_id, data=grouped, hue='language_model_id', style='language_model_id', markers=['s', 'v', 'o'], palette=[ \"#0077B3\", \"#FF1A75\",\"#33CC33\"], dashes=False, ci='sd')\n",
        "    ax.set_xscale('log')\n",
        "    ax.set_xlabel('Probe Rank')\n",
        "    ax.set_ylabel(y_label)\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    ax.legend(handles=handles[1:], labels=labels[1:])\n",
        "    ax.xaxis.set_major_locator(matplotlib.ticker.FixedLocator(training_config.ranks))\n",
        "    ax.xaxis.set_major_formatter(matplotlib.ticker.FixedFormatter(training_config.ranks))\n",
        "    out_path = os.path.join(OUT_RESULTS_IMAGES_ROOT, f'max_{y_label}_by_rank.png')\n",
        "    plt.savefig(out_path)\n",
        "\n",
        "def graph_y_by_hidden_layer_id(results_frame, y_id, y_label, agg_function: str):\n",
        "    ensure_dir(OUT_RESULTS_IMAGES_ROOT)\n",
        "    if agg_function == 'mean':\n",
        "        grouped = results_frame.groupby(['language_model_id', 'hidden_layer_id', 'seed'])[y_id].mean().reset_index()\n",
        "    elif agg_function == 'max':\n",
        "        grouped = results_frame.groupby(['language_model_id', 'hidden_layer_id', 'seed'])[y_id].max().reset_index()\n",
        "    elif agg_function == 'no_agg':\n",
        "        grouped = results_frame\n",
        "    plt.clf()\n",
        "    ax = sns.lineplot(x='hidden_layer_id', y=y_id, data=grouped, hue='language_model_id', style='language_model_id', markers=['s', 'v', 'o'], palette=[ \"#0077B3\", \"#FF1A75\",\"#33CC33\"], dashes=False, ci='sd')\n",
        "    ax.set_xlabel('Hidden Layer Index')\n",
        "    ax.set_ylabel(y_label)\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    ax.legend(handles=handles[1:], labels=labels[1:])\n",
        "    out_path = os.path.join(OUT_RESULTS_IMAGES_ROOT, f'average_{y_label}_by_hidden_layer_id.png')\n",
        "    plt.savefig(out_path)\n",
        "\n",
        "def plot_uuas_by_sentence_length(results_frame):\n",
        "    ensure_dir(OUT_RESULTS_IMAGES_ROOT)\n",
        "    plot_frame = results_frame.sort_values(by='language_model_id')\n",
        "    plt.clf()\n",
        "    ax = sns.lineplot(x='sentence_length', y='uuas_score', data=plot_frame, hue='language_model_id', style='language_model_id', markers=['s', 'v', 'o'], palette=[ \"#0077B3\", \"#FF1A75\",\"#33CC33\"], dashes=False, ci='sd')\n",
        "    ax.set_xlabel('Sentence Length')\n",
        "    ax.set_ylabel('UUAS')\n",
        "    ax.set_ylim(0.2, 1.0)\n",
        "    ax.set_xlim(2, 82)\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    ax.legend(handles=handles[1:], labels=labels[1:])\n",
        "    out_path = os.path.join(OUT_RESULTS_IMAGES_ROOT, f'uuas_by_sentence_length_most_selective_probes.png')\n",
        "    plt.savefig(out_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9RCycAtOpJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "STRUCTURAL_PROBE_REGULAR_TASK_RESULTS = STRUCTURAL_PROBE_RESULTS[STRUCTURAL_PROBE_RESULTS['task'] == 'regular']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLFCGdpzxm3y",
        "colab_type": "text"
      },
      "source": [
        "Scores of the most selective probe at the final hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6zVER6yxmCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FINAL_LAYER_SCORES = STRUCTURAL_PROBE_REGULAR_TASK_RESULTS.groupby(['language_model_id', 'hidden_layer_id', 'rank'], as_index=False)\n",
        "FINAL_LAYER_SCORES = FINAL_LAYER_SCORES.agg({'uuas_score': ['mean', 'std'],'generalized_selectivity': ['mean', 'std']})\n",
        "FINAL_LAYER_SCORES = FINAL_LAYER_SCORES.sort_values(by=['hidden_layer_id', ('generalized_selectivity', 'mean')], ascending=False)\n",
        "FINAL_LAYER_SCORES.columns = ['language_model_id', 'hidden_layer_id', 'rank', 'mean_uuas', 'mean_std', 'mean_gs', 'std_gs']\n",
        "FINAL_LAYER_SCORES = FINAL_LAYER_SCORES.drop_duplicates(subset=['language_model_id'])\n",
        "FINAL_LAYER_SCORES"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cW1sMUwuD1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qfzgalX7GFK",
        "colab_type": "text"
      },
      "source": [
        "Now let's graph!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpGjgoUeNP-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "graph_y_by_hidden_layer_id(STRUCTURAL_PROBE_REGULAR_TASK_RESULTS, 'generalized_selectivity', 'Generalized Selectivity', 'max')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xip4p5bs8v0k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "graph_y_by_hidden_layer_id(STRUCTURAL_PROBE_REGULAR_TASK_RESULTS, 'uuas_score', 'UUAS', 'max')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhiG6Qe7MuRH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "graph_y_by_rank(STRUCTURAL_PROBE_REGULAR_TASK_RESULTS, 'generalized_selectivity', 'Generalized Selectivity', STRUCTURAL_PROBE_TRAINING_CONFIG, 'mean')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sG5vS1JnBBWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "graph_y_by_rank(STRUCTURAL_PROBE_REGULAR_TASK_RESULTS, 'uuas_score', 'UUAS', STRUCTURAL_PROBE_TRAINING_CONFIG, 'max')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYwaguTh9Ryz",
        "colab_type": "text"
      },
      "source": [
        "Display highest scores and corresponding hidden layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDMBZ6ICsia9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AGG_SCORES_PER_LANGUAGE_MODEL = STRUCTURAL_PROBE_REGULAR_TASK_RESULTS.groupby(['language_model_id', 'hidden_layer_id', 'rank'], as_index=False)\n",
        "AGG_SCORES_PER_LANGUAGE_MODEL = AGG_SCORES_PER_LANGUAGE_MODEL.agg({'uuas_score': ['mean', 'std'], 'selectivity': ['mean', 'std'], 'generalized_selectivity': ['mean', 'std']})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LK3YcPCnghvk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BEST_UUUAS_PER_LANGUAGE_MODEL = AGG_SCORES_PER_LANGUAGE_MODEL.sort_values(by=('uuas_score', 'mean'), ascending=False)\n",
        "BEST_UUUAS_PER_LANGUAGE_MODEL.columns = ['language_model_id', 'hidden_layer_id', 'rank', 'mean_uuas', 'mean_std', 'mean_s', 'std_s', 'mean_gs', 'std_gs']\n",
        "BEST_UUUAS_PER_LANGUAGE_MODEL = BEST_UUUAS_PER_LANGUAGE_MODEL.drop_duplicates(subset=['language_model_id'])\n",
        "BEST_UUUAS_PER_LANGUAGE_MODEL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9j2eE_XthdeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BEST_SELECTIVITY_PER_LANGUAGE_MODEL = AGG_SCORES_PER_LANGUAGE_MODEL.sort_values(by=('selectivity', 'mean'), ascending=False)\n",
        "BEST_SELECTIVITY_PER_LANGUAGE_MODEL.columns = ['language_model_id', 'hidden_layer_id', 'rank', 'mean_uuas', 'mean_std', 'mean_s', 'std_s', 'mean_gs', 'std_gs']\n",
        "BEST_SELECTIVITY_PER_LANGUAGE_MODEL = BEST_SELECTIVITY_PER_LANGUAGE_MODEL.drop_duplicates(subset=['language_model_id'])\n",
        "BEST_SELECTIVITY_PER_LANGUAGE_MODEL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EE3EuejxhsHT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BEST_GSELECTIVITY_PER_LANGUAGE_MODEL = AGG_SCORES_PER_LANGUAGE_MODEL.sort_values(by=('generalized_selectivity', 'mean'), ascending=False)\n",
        "BEST_GSELECTIVITY_PER_LANGUAGE_MODEL.columns = ['language_model_id', 'hidden_layer_id', 'rank', 'mean_uuas', 'mean_std', 'mean_s', 'std_s', 'mean_gs', 'std_gs']\n",
        "BEST_GSELECTIVITY_PER_LANGUAGE_MODEL = BEST_GSELECTIVITY_PER_LANGUAGE_MODEL.drop_duplicates(subset=['language_model_id'])\n",
        "BEST_GSELECTIVITY_PER_LANGUAGE_MODEL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtmvPy0I9Xvb",
        "colab_type": "text"
      },
      "source": [
        "Measure probe generalization by plotting average UUAS by sentence length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9q7H21qb9XK6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "to_frame = {\n",
        "    'language_model_id': [],\n",
        "    'seed': [],\n",
        "    'sentence_length': [],\n",
        "    'uuas_score': []\n",
        "}\n",
        "\n",
        "LANGUAGE_MODEL_TO_DIM_MAP ={\n",
        "    'lstm': 650,\n",
        "    'xlnet': 1024,\n",
        "    'distilgpt2': 768\n",
        "}\n",
        "\n",
        "\n",
        "for tup in BEST_GSELECTIVITY_PER_LANGUAGE_MODEL.itertuples():\n",
        "    for seed in STRUCTURAL_PROBE_TRAINING_CONFIG.seeds:\n",
        "        probe_config = ProbeConfig(\n",
        "            language_model_id=tup.language_model_id,\n",
        "            corpus_id=CORPUS.id,\n",
        "            task='regular',\n",
        "            hidden_layer_id=tup.hidden_layer_id,\n",
        "            dim=LANGUAGE_MODEL_TO_DIM_MAP[tup.language_model_id],\n",
        "            rank=tup.rank,\n",
        "            seed=seed)\n",
        "        best_results = probe_config.load_test_set_results()\n",
        "        for tup2 in best_results.itertuples():\n",
        "            to_frame['language_model_id'].append(tup.language_model_id)\n",
        "            to_frame['sentence_length'].append(tup2.gold_distances.shape[0])\n",
        "            to_frame['seed'].append(seed)\n",
        "            to_frame['uuas_score'].append(tup2.uuas_score)\n",
        "\n",
        "HIGHEST_GS_PROBES_UUAS_BY_SENTENCE_LENGTH = pd.DataFrame(to_frame)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z17T1JTbrws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_uuas_by_sentence_length(HIGHEST_GS_PROBES_UUAS_BY_SENTENCE_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb41ct6U9lZC",
        "colab_type": "text"
      },
      "source": [
        "Finally, let's test for significance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29aLQrAueBD1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_scores = HIGHEST_GS_PROBES_UUAS_BY_SENTENCE_LENGTH[HIGHEST_GS_PROBES_UUAS_BY_SENTENCE_LENGTH['language_model_id'] == 'lstm'].groupby(['sentence_length']).agg({'uuas_score': 'mean'}).values\n",
        "distilgpt2_scores = HIGHEST_GS_PROBES_UUAS_BY_SENTENCE_LENGTH[HIGHEST_GS_PROBES_UUAS_BY_SENTENCE_LENGTH['language_model_id'] == 'distilgpt2'].groupby(['sentence_length']).agg({'uuas_score': 'mean'}).values\n",
        "xlnet_scores = HIGHEST_GS_PROBES_UUAS_BY_SENTENCE_LENGTH[HIGHEST_GS_PROBES_UUAS_BY_SENTENCE_LENGTH['language_model_id'] == 'xlnet'].groupby(['sentence_length']).agg({'uuas_score': 'mean'}).values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg9HJudogrIz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ttest_ind(lstm_scores, distilgpt2_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHoWaSRfgn-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ttest_ind(lstm_scores, xlnet_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXGz4WJAgtkE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ttest_ind(distilgpt2_scores, xlnet_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYw6Z4Solz36",
        "colab_type": "text"
      },
      "source": [
        "Let's also test significance between final layer results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8bS_UVrl2ut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FINAL_LAYER_SCORES_BY_SEED = STRUCTURAL_PROBE_REGULAR_TASK_RESULTS.groupby(['language_model_id', 'hidden_layer_id','seed', 'rank'], as_index=False)\n",
        "FINAL_LAYER_SCORES_BY_SEED = FINAL_LAYER_SCORES_BY_SEED.agg({'uuas_score': ['mean'],'generalized_selectivity': ['mean']})\n",
        "FINAL_LAYER_SCORES_BY_SEED_XLNET = FINAL_LAYER_SCORES_BY_SEED[(FINAL_LAYER_SCORES_BY_SEED.language_model_id == 'xlnet') & (FINAL_LAYER_SCORES_BY_SEED.hidden_layer_id == 12)]\n",
        "FINAL_LAYER_SCORES_BY_SEED_LSTM = FINAL_LAYER_SCORES_BY_SEED[(FINAL_LAYER_SCORES_BY_SEED.language_model_id == 'lstm') & (FINAL_LAYER_SCORES_BY_SEED.hidden_layer_id == 1)]\n",
        "FINAL_LAYER_SCORES_BY_SEED_DISTILGPT2 = FINAL_LAYER_SCORES_BY_SEED[(FINAL_LAYER_SCORES_BY_SEED.language_model_id == 'distilgpt2') & (FINAL_LAYER_SCORES_BY_SEED.hidden_layer_id == 6)]\n",
        "FINAL_LAYER_SCORES_BY_SEED = pd.concat([FINAL_LAYER_SCORES_BY_SEED_XLNET, FINAL_LAYER_SCORES_BY_SEED_LSTM, FINAL_LAYER_SCORES_BY_SEED_DISTILGPT2])\n",
        "FINAL_LAYER_SCORES_BY_SEED = FINAL_LAYER_SCORES_BY_SEED.sort_values(by=('generalized_selectivity', 'mean'), ascending=False)\n",
        "FINAL_LAYER_SCORES_BY_SEED.columns = ['language_model_id', 'hidden_layer_id', 'seed', 'rank', 'mean_uuas', 'mean_gs']\n",
        "FINAL_LAYER_SCORES_BY_SEED = FINAL_LAYER_SCORES_BY_SEED.drop_duplicates(subset=['language_model_id', 'seed'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SbjI2xzmiZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "to_frame = {\n",
        "    'language_model_id': [],\n",
        "    'seed': [],\n",
        "    'uuas_score': []\n",
        "}\n",
        "\n",
        "for tup in FINAL_LAYER_SCORES_BY_SEED.itertuples():\n",
        "    probe_config = ProbeConfig(\n",
        "        language_model_id=tup.language_model_id,\n",
        "        corpus_id=CORPUS.id,\n",
        "        task='regular',\n",
        "        hidden_layer_id=tup.hidden_layer_id,\n",
        "        dim=LANGUAGE_MODEL_TO_DIM_MAP[tup.language_model_id],\n",
        "        rank=tup.rank,\n",
        "        seed=seed)\n",
        "    best_results = probe_config.load_test_set_results()\n",
        "    for tup2 in best_results.itertuples():\n",
        "        to_frame['language_model_id'].append(tup.language_model_id)\n",
        "        to_frame['seed'].append(seed)\n",
        "        to_frame['uuas_score'].append(tup2.uuas_score)\n",
        "\n",
        "FINAL_LAYER_SCORES_BY_SEED_BY_SENTENCE = pd.DataFrame(to_frame)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb9-fpdLs4b3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_uuas = ('lstm', FINAL_LAYER_SCORES_BY_SEED_BY_SENTENCE[(FINAL_LAYER_SCORES_BY_SEED_BY_SENTENCE['language_model_id'] == 'lstm')]['uuas_score'].values)\n",
        "distilgpt2_uuas = ('distilgpt2', FINAL_LAYER_SCORES_BY_SEED_BY_SENTENCE[(FINAL_LAYER_SCORES_BY_SEED_BY_SENTENCE['language_model_id'] == 'distilgpt2')]['uuas_score'].values)\n",
        "xlnet_uuas = ('xlnet', FINAL_LAYER_SCORES_BY_SEED_BY_SENTENCE[(FINAL_LAYER_SCORES_BY_SEED_BY_SENTENCE['language_model_id'] == 'xlnet')]['uuas_score'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ix-Pgmp3tC7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for combo in itertools.combinations([lstm_uuas, distilgpt2_uuas, xlnet_uuas], 2):\n",
        "    (model_name_one, results_one), (model_name_two, results_two) = combo\n",
        "    sig_val = ttest_ind(results_one, results_two).pvalue\n",
        "    print(f'The difference between {model_name_one} and {model_name_two} is {\"not\" if sig_val > 0.05 else \"indeed\" } significant: {sig_val}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E51v6IQp9nfm",
        "colab_type": "text"
      },
      "source": [
        "# Generate Final Report Figures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-n_UBEM_Nss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_pos_tag_results_for_paper(data, lims = [(0.78, 0.92), (0.15, 0.3)], step_size = [0.03, 0.03], x_label = \"Sample count\", x_ticks=[100, 1000, 10000, 12543]):\n",
        "\n",
        "    y_label = [\"Mean accuracy\", \"Mean selectivity\"]\n",
        "\n",
        "    layout=[(\"s\", \"#0077b3\"), (\"o\", \"#33cc33\"), (\"v\",\"#ff1a75\")]\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(9,3))\n",
        "\n",
        "    for i, d in enumerate(data): \n",
        "        for j, (model_id, res) in enumerate(d.items()):\n",
        "            y = np.array(res[\"mean\"])\n",
        "            std = np.array(res[\"std\"])\n",
        "\n",
        "            # Make sure does not exteed 0 - 1 \n",
        "            std_plus = y+std #np.where(y+std > 1, 1, y+std)\n",
        "            std_min = y-std #np.where(y-std < 0, 0, y-std)\n",
        "\n",
        "            marker = layout[j][0]\n",
        "            color = layout[j][1]\n",
        "\n",
        "            x = range(len(y))\n",
        "            # Plot\n",
        "            ax[i].plot(x, y, color=color, marker = marker, markersize=7, label=model_id)\n",
        "            ax[i].fill_between(x, std_min, std_plus, facecolor=color,alpha=0.2, interpolate=True)\n",
        "\n",
        "            ax[i].set_ylabel(y_label[i], fontsize=15)\n",
        "            ax[i].set_xlabel(x_label, fontsize=14)\n",
        "            ax[i].set_ylim(lims[i]) \n",
        "            ax[i].set_xlim((0, 1)) \n",
        "\n",
        "            ax[i].set_xticks(x)\n",
        "            ax[i].set_xticklabels(x_ticks, fontsize=14)\n",
        "            ax[i].set_yticks(np.arange(lims[i][0], lims[i][1]+step_size[i], step_size[i]))\n",
        "            ax[i].tick_params(axis=\"y\", labelsize=13)\n",
        "            ax[0].legend(prop={'size': 13}, loc=\"lower right\")\n",
        "    plt.suptitle(\"Part-of-Speech tag prediction\", fontsize=15)\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.93])\n",
        "    plt.savefig(os.path.join(OUT_RESULTS_ROOT, \"plot_pos_tag_hidden_indices.png\"))\n",
        "    plt.show()\n",
        "\n",
        "def plot_tree_distance_task_results_for_paper(tree_distance_results_frame):\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(8,3))\n",
        "\n",
        "    grouped = tree_distance_results_frame.groupby(['language_model_id', 'hidden_layer_id', 'seed'])['uuas_score'].max().reset_index()\n",
        "\n",
        "    sns.lineplot(ax=ax[0], x='hidden_layer_id', y='uuas_score', data=grouped, hue='language_model_id', style='language_model_id',markersize=8, markers=['s', 'v', 'o'], palette=[ \"#0077B3\", \"#FF1A75\",\"#33CC33\"], dashes=False, ci='sd')\n",
        "    handles, labels = ax[0].get_legend_handles_labels()\n",
        "    ax[0].legend(handles=handles[1:], labels=labels[1:], prop={'size': 11}, loc=\"lower right\")\n",
        "    ax[0].set_xlabel('Hidden Layer Index', fontsize=14)\n",
        "    ax[0].set_ylabel('UUAS', fontsize=15)\n",
        "\n",
        "    # ax[0].set_ylim((0.35, 0.75)) \n",
        "\n",
        "    ax[0].set_xticks(np.arange(13))\n",
        "    ax[0].set_xticklabels(np.arange(13), fontsize=13)\n",
        "    # ax[0].set_yticks(np.arange(0.35, 0.75+0.1, 0.1))\n",
        "    ax[0].tick_params(axis=\"y\", labelsize=13)\n",
        "\n",
        "\n",
        "    grouped = tree_distance_results_frame.groupby(['language_model_id', 'hidden_layer_id', 'seed'])['generalized_selectivity'].max().reset_index()\n",
        "\n",
        "    sns.lineplot(ax=ax[1], x='hidden_layer_id', y='generalized_selectivity', data=grouped, legend=None, hue='language_model_id', style='language_model_id',markersize=8, markers=['s', 'v', 'o'], palette=[ \"#0077B3\", \"#FF1A75\",\"#33CC33\"], dashes=False, ci='sd')\n",
        "    ax[1].set_xlabel('Hidden Layer Index', fontsize=14)\n",
        "    ax[1].set_ylabel('Gen. Selectivity', fontsize=15)\n",
        "\n",
        "    # ax[1].set_ylim((-0.1, 0.3)) \n",
        "\n",
        "    ax[1].set_xticks(np.arange(13))\n",
        "    ax[1].set_xticklabels(np.arange(13), fontsize=12)\n",
        "    # ax[1].set_yticks(np.arange(-0.1, 0.3 +0.1, 0.1))\n",
        "    ax[1].tick_params(axis=\"y\", labelsize=13)\n",
        "\n",
        "    plt.suptitle(\"Tree Distance prediction\", fontsize=14)\n",
        "    # plt.tight_layout(rect=[0, 0.03, 1, 0.94])\n",
        "    plt.savefig(os.path.join(OUT_RESULTS_ROOT, \"plot_tree_distance_hidden_indices.png\"))\n",
        "    plt.show()   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pmJumKB_GXi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if RUN_POS_TASK:\n",
        "    plot_pos_tag_results_for_paper(ALL_POS_RESULTS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5Ix19rLEEJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if RUN_STRUCTURAL_TASK:\n",
        "    plot_tree_distance_task_results_for_paper(STRUCTURAL_PROBE_REGULAR_TASK_RESULTS)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}